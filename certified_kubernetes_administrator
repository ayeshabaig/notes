Certified Kubernetes Administrator

Set up your practice cluster
Playground
Ubuntu, North America, Medium, Kube Master, Kube Node 1, Kube Node 2

ssh into VMs

Add the Docker Repository on all three servers.
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository    "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"

Add the Kubernetes repository on all three servers.
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF

Install Docker, Kubeadm, Kubelet, and Kubectl on all three servers.
sudo apt-get update
sudo apt-get install -y docker-ce=18.06.1~ce~3-0~ubuntu kubelet=1.12.2-00 kubeadm=1.12.2-00 kubectl=1.12.2-00
sudo apt-mark hold docker-ce kubelet kubeadm kubectl

Enable net.bridge.bridge-nf-call-iptables on all three nodes.
echo "net.bridge.bridge-nf-call-iptables=1" | sudo tee -a /etc/sysctl.conf
sudo sysctl -p

On only the Kube Master server, initialize the cluster and configure kubectl.
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Install the flannel networking plugin in the cluster by running this command on the Kube Master server.
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml

The kubeadm init command that you ran on the master should output a kubeadm join command containing a token and hash. You will need to copy that command from the master and run it on both worker nodes with sudo.
sudo kubeadm join $controller_private_ip:6443 --token $token --discovery-token-ca-cert-hash $hash

Now you are ready to verify that the cluster is up and running. On the Kube Master server, check the list of nodes.
kubectl get nodes

It should look something like this:
NAME                      STATUS   ROLES    AGE   VERSION
wboyd1c.mylabserver.com   Ready    master   54m   v1.12.2
wboyd2c.mylabserver.com   Ready    <none>   49m   v1.12.2
wboyd3c.mylabserver.com   Ready    <none>   49m   v1.12.2

Make sure that all three of your nodes are listed and that all have a STATUS of Ready.

Architecture Diagram & Explanation
https://linuxacademy.com/cp/guides/download/refsheets/guides/refsheets/linuxacademy-kubernetesadmin-archdiagrams-1_1516737832.pdf
Master Node
  Kube-APISERVER
    ETCD
    Kube-Scheduler
    Cloud-Controller-Manager <-> Cloud
    Kube-Controller-Manager
Node
  Kubelet
  Kube Proxy
  Pod
    Container

How to install Kubernetes on Centos
Create 3 VMs as with Ubuntu install

Turn off swap on all servers.
sudo swapoff -a
sudo vi /etc/fstab
Look for the line in /etc/fstab that says /root/swap and add a # at the start of that line, so it looks like: #/root/swap. Then save the file.

Install and configure Docker.
sudo yum -y install docker
sudo systemctl enable docker
sudo systemctl start docker

Add the Kubernetes repo.
cat << EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

Turn off selinux.
sudo setenforce 0
sudo vi /etc/selinux/config

Change the line that says SELINUX=enforcing to SELINUX=permissive and save the file.

Install Kubernetes Components.
sudo yum install -y kubelet kubeadm kubectl
sudo systemctl enable kubelet
sudo systemctl start kubelet

Configure sysctl.
cat << EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sudo sysctl --system

Initialize the Kube Master. Do this only on the master node.
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Install flannel networking.
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml

The kubeadm init command that you ran on the master should output a kubeadm join command containing a token and hash. You will need to copy that command from the master and run it on all worker nodes with sudo.
sudo kubeadm join $controller_private_ip:6443 --token $token --discovery-token-ca-cert-hash $hash

Now you are ready to verify that the cluster is up and running. On the Kube Master server, check the list of nodes.
kubectl get nodes

It should look something like this:
NAME                      STATUS   ROLES    AGE     VERSION
wboyd4c.mylabserver.com   Ready    master   3m36s   v1.12.2
wboyd5c.mylabserver.com   Ready    <none>   23s     v1.12.2
Make sure that all of your nodes are listed and that all have a STATUS of Ready.

Kubernetes API Primitives (objects) & Cluster Architecture
API Primtives
- Persistent entities in the Kubernetes System.
- Uses these to represent the state of the cluster.
- Describe:
    What applications are running
    Which nodes those applications are running on.
    Policies around those applications.
- Kubernetes Objects are "records of intent"

Object Spec:
- Provided to Kubernetes
- Describes desired state of objects

Object Status:
- Provided by Kubernetes
- Describes the actual state of the object

Kubernetes Yaml
kubectl turns yaml into json
apiVersion: v1 -> # required
kind: Pod # what kind of object is being described
metadata: # data used to uniquely identify object (maybe name, uid, namespace
  name: busybox
spec: # format of spec field is different for every object, kube api reference has format rules
  containers:
  - name: busybox
    image: busybox
    command:
      - sleep
      - "3600"

Common Kube Objects
- Nodes # hosts that make up cluster
- Pods # single instances of application containers
- Deployments # sets of load-balanced pods
- Services # expose deployments to external networks
- ConfigMaps # key-value pairs that can be plugged into other objects as needed

Names and UIDs
Names
- All objects have a unique name.
- Client provided.
- Can be reused.
- Maximum length of 253 chars
- Lower case alphanumeric chars
- - and . allowed
UIDs
- All objects have a unique UID.
- Generated by Kubernetes.
- Spatially and temporally unique.

Namespaces
- Multiple virtual clusters backed by the same virtual cluster
- Generally for large deployments.
- Provide scope for names.
- Easy way to divide cluster resources.
- Allows for multiple teams of users.
- Allows for resource quotas.
- Special 'kube-system' namespace
    Used to differentiate system pods from user pods.

Nodes
- Might be a VM or physical machine.
- Services necessary to run pods.
- Managed by the master.
- Services necessary
    Container runtime
    Kubelet
    Kube-proxy
- Not inherently created by Kubernetes, but by Cloud Provide
- Kubernetes checks the node for validity.

Cloud Controller Managers
- Route controller (gce clusters only)
- Service Controller
- PersistentVolumeLabels controller

Node Controller
- Assigns CIDR block to a newly registered node.
- Keeps track of the nodes.
- Monitors the node health.
- Evicts pods from unhealthy nodes
- Can taint nodes based on current conditions in more recent versions.

Kubernetes Services
- Underlying architecture
- Pod -- Simplest kubernetes object, represents one or more containers running on a single node.
- Ephemeral, disposable, and replaceable - Stateless
- "Cattle vs. Pets"
- Usually managed via Deployments

- Deployment specifications - refer to details of pods like images and number of replicas

Services refer to Deployments and expose a particular port of IP address

Running the application pods.
  How you set up a service depends on your networking configuration
  and how you will handle load balancing and port forwarding.
  If you use the pod network IP address method, then a deployment gets
  assigned a single IP address -- even if there are multiple replicas of that pod.
  The kubernetes service (using kube-proxy on the node) redirects traffic.

Commands can be imperative or declarative
Imperative
  kubectl run nginx --image=nginx
Declarative
  apiVersion: apps/v1
  kind: Deployment
  metadata: ...

Points to remember:
- Containers are run in Pods, the simplest Kubernetes object representing an application.
- Pods are (usually) managed by deployments.
- Services expose deployments.
- Third parties handle load balancing or port forwarding to those services,
  though Ingress objects (along with an appropriate ingress controller) are needed to do that work.

sample yaml for a job which uses perl to calculate pi to 2000 digits and stops
---
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
  backoffLimit: 4

yaml for a job to create a busybox pod that sleeps for 10 seconds and stops
---
apiVersion: batch/v1
kind: Job
metadata:
  name: busybox
spec:
  template:
    spec:
      containers:
      - name: busybox
        image: busybox
        command: ["sleep", "10"]
      restartPolicy: Never
  backoffLimit: 4

kubectl describe job pi -> use that to get pod name and run
kubectl logs [pod-name]

This pod will cause the alpine linux container to sleep for 3600 seconds (1 hour) and then exit. Kubernetes will then restart the pod.
---
apiVersion: v1
kind: Pod
metadata:
  name: alpine
  namespace: default
spec:
  containers:
  - name: alpine
    image: alpine
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always

To create the pod: kubectl create -f alpine.yaml
To delete the pod: kubectl delete -f alpine.yaml
  or kubectl delete pod alpine or kubectl delete pod/alpine

yaml example for nginx pod:
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  -  name: nginx
      image: nginx
  restartPolicy: Always

Designing a Kubernetes Cluster
Use the kubernetes cluser on linux academy playground
minikube- single node on local- linux, windows, mac
kubeadm to deploy multinode locally, need your own CNI (flannel)
Ubuntu on LXD provides nine-instance supported kube cluster on local
cloud providers - google, azure, stackpoint, etc
turnkey solutions - up and running fast - google, aws, oracle
add-on solutions- most important is CNI Container Networking Interface
  Calico is a secure L3 networking and network policy provider
  Canal unites Flannel and Calico, providing networking and network policy
  Cilium is a L3 network and network policy plugin that can enforce HTTP/APL/L7 policies transparently
    Both routing and overlay/encapsulation mode are supported.
  CNI-Genie enables Kubernetes to seamlessly connect to a choice of CNI plugins,
    such as Calico, Canal, Flannel, Romana, or Weave
Contiv
  provides configurable networking (native L3 using BGP, overlay using vxlan, classic L2, and Cisco-SDN/ACI)
  for various uses and rich policy framework.
  fully open sourced
  installer provides both kubeadm and non-kubeadm based installation options
Flannel is an overlay network provider that can be used with Kubernetes, and is the one we're using in our LA servers
Multus is a Multi plugin for multiple network support in Kubernetes to support all CNI plugins
  (e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, DPDK, OVS-DPDK, and VPP based workloads in Kubernetes
NSX-T Container Plug-in (NCP) provides integration between VMWare NSX-T and container orchestrators such as Kube
  Also provides integrations between NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service PKS and Openshift
Nuage is an SDN platform that provide policy-based networking between Kubernetes Pods and
  non-Kube environments with visibility and security monitoring
Romana is a Layer 3 networking solution for pod networks that also supports the NetworkPolicy API
Weave Net provides networking and network policy, will carry on working on both sides of a network partition,
  and does not require an external database
There are also add-ons for service discovery and visualization and control, like the dashboard.

Exercise: Explore the Sandbox
1. Examine the current status of your cluster. Are all the nodes ready? How do you know?
The command kubectl get nodes will give the current status of all nodes.

2. Are there any pods running on node 2 of your cluster? How can you tell?
You can get this information in a variety of ways:
kubectl describe node node-name
kubectl get pods --all-namespaces -o wide will list all pods and which nodes they are currently running on.

3. Is the master node low on memory currently? How can you tell?
kubectl describe node node-2-name will list DiskPressure and MemoryPressure statuses so you can see how it is doing.

4. What pods are running in the kube-system namespace? What command did you use to find out?
kubectl get pods -n kube-system will provide the desired results.

Hardware and Underlying Infrastructure
The hardware and infrastructure that k8s will run on is truly staggering
Many developers run full, 6 node k8s and docker clusters on raspberry pis
Nodes, including the master, can be physical or virtual machines running k8s componenets
  and a container manager such as docker or rocket
The nodes should be connected by a common network, thought he internet will work as long as
  port 443 and whatever your pod networking system uses are unblocked
A pod networking application such as Flannel is needed to allow the pods to communicate
  with one another, and it makes use of an overlay network (by default it's vxlans)
  to provide that service
The Linux Academy Kubernetes Lab you built on Cloud Servers in an earlier lab, for example,
  is made up of two to six VMs. We then used kubeadm to bootstrap the master, and ran
  a join command on each of the nodes to provision them.
For the exam, it's important to understand this relationship.

Cluster Communications
- Cluster communications cover communications to the API server, control-plane communications inside the cluster,
  and can even include pod-to-pod communications.
- This is an in-depth topic with a fair amount of detail, so let's start by discussing how to secure
  communications to the Kubernetes API server.
- Everything in Kubernetes goes through the API driver, so controlling and limiting who has access
  to the cluster and what they are allowed to do is arguably the most important task in securing the cluster.
- The default encryption communication in K8s is TLS
- Most of the installation methods will handle the certificate creation.
- Kubeadm created certificates during the Linux Academy Cloud Server Kubernetes Cluster Lab
- No matter how you've installed k8s, some components and installation methods may enable
  local ports over HTTP and you should double check the settings of these components to
  identify potentially unsecured traffic and address these issues.
- Anything that connects to the API, including nodes, proxies, the scheduler, volume plugins in additions
  to users, should be authenticated.
- Again, most installation methods create certificates for those infrastructure pieces, but if you've chosen to install manually,
  you might need to do this yourself.
- Once authenticated, every API call should pass an authorization check.
RBAC
- Kubernetes has an integrated Role-Based Access Control (RBAC) component
- Certain roles perform specific actions in the cluster
- Kubernetes has several well thought out, pre-created roles
- Simple rules might be fine for smaller clusters

- If a user doesn't have rights to perform and action, but they do have access to perform a composite
  action that includes it, the user WILL be able to indirectly create objects.
- Carefully consider what you want users to be allowed to do prior to making changes to the existing roles.

Securing the Kubelet
- Secure the kubelet on each node.
- The Kubelets expose HTTPS endpoints which give access to both data and actions on the nodes.
  By default, these are open.
- To secure those endpoints, you can enable Kubelet Authentication and Authorization by starting it
  with an --anonymous-auth=false flag and assigning it an appropriate x509 client certificate in its configuration.

Securing the Network
- Network Policies restrict access to the network for a particular namespace.
- This allows developers to restrict which pods in other namespaces can access pods and port within
  the current namespace.
- The pod networking CNI must respect these policies which, fortunately, most of them do
- Users can also be assigned quotas or limit ranges
- Use Plug-Ins for more advanced functionality
- That should secure all the communications in a cluster

Vulnerabilities
- Kubernetes makes extensive use of etcd for storing configuration and secrets. It acts as the
  key/value store for the entire cluster.
- Gaining write access to etcd is very much like gaining root on the whole cluster, and even
  read access can be used by attackers to cause some serious damage.
- Strong credentials on your etcd server or cluster is a must.
- Isolate those behind a firewall that only allows requests from the API servers.

- Audit logging is also critical
- Records actions taken by the API for later analysis in the event
- Enable audit logging and archive the audit file on a secure server

- Rotate your infrastructure credentials frequently
- Smaller lifetime windows for secrets and credentials create bigger problems for attackers attempting to use it.
- You can even set these up to have short lifetimes and automate their rotation.

Third Party Integrations
- Always review third party integrations before enabling them.
- Integrations to Kubernetes can change how secure your cluster is.
- Add-ons might be nothing more than just more pods in the cluster, but those can be powerful.
- Don't allow them into the kube-system namespace

- You should also join the kubernetes-announce group for emails about security announcements
  https://groups.google.com/forum/#!forum/kubernetes-announce

Making Kubernetes Highly Available
- The challenges associated with making any platform HA are many
- While the exam isn't going to make you deploy a full HA kubernetes cluster, we should take a look
  at the process from a high level to gain an understanding of how it should work

The Process
- Create the reliable nodes that will form our cluster.
- Set up a redundant and reliable storage service with a multinode deployment of etcd
- Start replicated and load balanced Kubernetes API servers.
- Set up a master-elected kubernetes scheduler and controller manager daemons.
- Here is what our final system is going to look like
- Notice that everything that needs the API server or any other service on the master goes through
  the load balancer, including the worker nodes.

Step One
- Make the Master node reliable
- Ensure that the services automatically restart if they fail
- Kubelet already does this, so it's a convenient piece to use
- If the kubelet goes down, though, we need to restart it
- Monit on Debian systems or systemctl on systemd-based systems.

Step Two - Storage Layer
- If we're going to make it HA, then we've got to make sure that persistent storage is rock solid
- Protect the data
- If you have the data, you can rebuild almost anything else.
- Once the data is gone, the cluster is gone too
- Clustered etcd already replicates the storage to all master instances in your cluster
- To lose data, all three nodes would need to have their disks fail at the same time.
- The probability of this is relatively low, so just running a replicated etcd cluster is reliable enough.
- Add additional reliability by increasing the size of the cluster from three to five nodes.
- A multinode cluster of etcd
- If you use a cloud provider, then they usually provide this for you.
- If you are running on physical machines, you can also use network attached redundant storage using
  an iSCSI or NFS. Alternatively, you can run a clustered file system like Gluster or Ceph.
- RAID array on each physical machine

Step Three - Replicated API Services
- Create the initial log file so that Docker will mount a file instead of a directory:
  touch /var/log/kube-apiserver.log
- Next, we create a /srv/kubernetes directory on each node which should include:
  basic_auth.csv - basic auth user and password
  ca.crt - Certificate Authority cert
  known_tokens.csv - tokens that entities (e.g. the kubelet) can use to talk to the apiserver
  kubecfg.crt - client certificate, public key
  kubecfg.key - client certificate, private key
  server.cert - server certificate, public key
  server.key - server certificate, private key
- Either create this manually or copy it from a master node on a working cluster.
- Once these files exist, copy the kube-apiserver.yaml into /etc/kubernetes/manifests/ on each of our master nodes
- the kubelet monitors this directory, and will automatically create an instance of the
  kube-apiserver container using the pod definition specified in the file.
- If a network load balancer is set up, then access the cluster using the VIP and see traffic
  balancing between the apiserver instances.
- Setting up a load balancer will depend on the specifics of your platform.
- For external users of the API (e.g., the kubectl command line interface, continuous build pipelines, or other clients)
  remember to configure them to talk to the external load balancer's IP address.

Step Four - Controller/Scheduler Daemons
- Now we need to allow our state to change.
- Controller managers and schedulers.
- These processes must not modify the cluster's state simultaneously, use a lease-lock
- Each scheduler and controller manager can be launched with a --leader-elect flag
- The scheduler and controller-manager can be configured to talk to the API server that is on the
  same node (i.e. 127.0.0.1)
    It can also be configured to communicate using the load balanced IP address of the API servers.
- The scheduler and controller-manager will complete the leader election process mentioned before when
  using the --leader-elect flag
- In case of a failure accessing the API server, the elected leader will not be able to renew the lease,
  causing a new leader to be elected.
- This is especially relevant when configuring the scheduler and controller-manager to access the API server
  via 127.0.0.1, and the API server on the same node is unavailable
Installing Configuration Files
- Create empty log files on each node, so that Docker will mount the files and not make new directories:
  touch /var/log/kube-scheduler.log
  touch /var/log/kube-controller-manager.log
- Set up the descriptions of the scheduler and controller manager pods on each node by copying kube-scheduler.yaml
  and kube-controller-manager.yaml into the /etc/kubernetes/manifests/ directory
- And once that's all done, we've got our cluster highly available! Remember, if a worker node goes down,
  kubernetes will rapidly detect that and spin up replacement pods elsewhere in the cluster.

End-to-End Testing and Validation
- Provide a mechanism to test end-to-end behavior of the system
- Last signal to ensure end user operations match specifications
- Primarily a developer tool
- Difficult to run against just "any" deployment - many specific tests for cloud providers
    Ubuntu has its own Juju-deployed tests
    GCE has its own
    Many tests offered
Kubetest Suite
- Ideal for GCE or AWS users
- Build
- Stage
- Extract
- Bring up the cluster
- Test
- Dump logs
- Tear down
Conclusion
- e2e validation is primarily for developers creating new code
- Kubetest is a testing suite for Kubernetes e2e testing

Validating Nodes and the Cluster
Login to Kube Master
kubectl get nodes
kubectl describe node [server-name]
can also log into a worker node and run `ps aux | grep kube` to make sure
  kubelet, kube-proxy, and networking plugin are running

Deployments, Rolling Updates, and Rollbacks
nginx-deployment.yaml
---
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template: # describes the pods that are going to be created
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80

kubectl create -f nginx-deployment.yaml
kubectl get deployments
kubectl describe deployment nginx-deployment
kubectl get deployment nginx-deployment -o yaml -> way to get yaml back out

How to do a rollout
kubectl set image deployment nginx-deployment nginx=nginx:1.8
kubectl rollout status deployment/nginx-deployment
kubectl describe deployment nginx-deployment
vim nginx-deployment -> edit version to 1.9
kubectl apply -f nginx-deployment.yaml
kubectl rollout status deployment/nginx-deployment
kubectl describe deployment nginx-deployment
kubectl rollout history deployment/nginx-deployment --revision=3
kubectl rollout undo deployment/nginx-deployment --to-revision=2

Exercise: Deployments
1. Create the deployment.
  Create the yaml file and name it something. I chose nginx-deployment.yaml. Create the deployment object by calling:
  kubectl create -f nginx-deployment.yaml

2. Which nodes are the pods running on. How can you tell?
  There are many ways to get this, here's one example that gives you the results in one step and uses a label selector:
  kubectl get pods -l app=nginx -o wide

3. Update the deployment to use the 1.8 version of the nginx container and roll it out.
  There are many ways. Here are two:
  This will work just fine but is not the preferredmethod because now the yaml is inconsistent with what you've got running in the cluster. Anyonecomingacrossyouryaml will assume it's what is up and running and it isn't.
  kubectl set image deployment nginx-deployment nginx=nginx:1.8
  Updatethelineintheyaml to the 1.8 version of the image, and apply the changes with
  kubectl apply -f nginx-deployment.yaml

4. Update the deployment to use the 1.9.1 version of the nginx container and roll it out.
  Same as above. Don't forget you can watch the status of the rollout with the command

5. Roll back the deployment to the 1.8 version of the container.
  kubectl rollout status deployment nginx-deployment
  kubectl rollout undo deployment nginx-deployment
  will undo the previous rollout, or if you want to go to a specific point in history, you can view the history and roll back to a specific state with:
  kubectl rollout history deployment nginx-deployment
  kubectl rollout undo deployment nginx-deployment --to-revision=x

6. Remove the deployment
  kubectl delete -f nginx-deployment.yaml

How Kubernetes Configures Applications
ConfigMaps
kubectl create configmag my-map --from-literal=LinuxAcademy
kubectl get configmaps
kubectl describe configmaps my-map
kubectl get configmap my-map -o yaml
cat pod-config.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: config-test-pod
spec:
  containers:
    - name: test-container
      image: busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        - name: WHAT_SCHOOL
          valueFrom:
            configMapKeyRef:
              name: my-map
              key: school
  restartPolicy: never

kubectl create -f pod-config.yaml
kubectl logs config-test-pod

Exercise: Using ConfigMaps to Set Environment Variables
1. Write yaml for a ConfigMap that will create a data key called "myKey" with the value "myValue"
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config-map
data:
  myKey: myValue

2. Create the ConfigMap using the kubectl command-line tool
kubectl apply -f my-config-map.yaml to create the ConfigMap

3. Write a pod yaml to pass the ConfigMap data to the pod, so that the pod can use it as an environment variable
Here is a yaml file named "my-configmap-pod.yaml" that will pass the ConfigMap data to the pod so the container can use it as an environment variable:

apiVersion: v1
kind: Pod
metadata:
  name: my-configmap-pod
spec:
  containers:
  - name: myapp-container
    image: busybox
    command: ['sh', '-c', "echo $(MY_VAR) && sleep 3600"]
    env:
    - name: MY_VAR
      valueFrom:
        configMapKeyRef:
          name: my-config-map
          key: myKey

4. Create the pod using kubectl
kubectl apply -f my-configmap-pod.yaml to create a pod that will use the ConfigMap data as an environment variable

5. Echo the value of the environment variable to the console
kubectl logs my-configmap-pod to display the "echo $(MY_VAR)" command from the container

Scaling Applications
got a deployment with 2 replicas
to scale it to 3 replicas:
kubectl scale deployment/nginx-deployment --replicas=3
kubectl get deployments
kubectl get pods
kubectl scale deployment/nginx-deployment --replicas=1 -> to scale down
Autoscaling is possible with metrics and heapster, but not part of the exam

Scaling practice
---
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80

Scale the deployment up to 4 pods without editing the YAML.
kubectl scale deployment/nginx-deployment --replicas=4

Edit the YAML so that 3 pods are available and can apply the changes to the existing deployment.
edit the replicas line to 3 from 2 and
kubectl apply -f nginx-deployment.yaml

Which of these methods do you think is preferred and why?
  Performing the edit in the YAML is the preferred one,
  as it keeps the YAML on disk in sync with the state of the cluster.

Self-Healing Applications
all apps in kube are self-healing as kube changes things to match specs
kubectl get deployments
kubectl get pods
kubectl delete pod nginx-deployment-8989234
kubectl get pods -> a new pod is getting created
what if a node dies
shut off a node
kubectl get deployments -> shows 2 of 3 available
kubectl get pods -> still shows 3 pods, kube takes 5 mins to evict a pod it cant find
kubectl describe pod nginx-deployment-8989234 -> shows Tolerations: node-kubernetes.io/not-ready:NoExecute for 300s
kubectl get pods -> after 300s shows Status Unknown for the pod in node that was stopped
power back up the node -> shows ready -> no new pods started up, still at 3 replicas

Exercise: Replication Controllers, Replica Sets, and Deployments
Write a YAML file that will create a Replication Controller that will maintain three copies of an nginx container.
  Execute your YAML and make sure it works.
A Replica Set is a more advanced version of a Replication Controller that is used when more low-level control is needed.
  While these are commonly managed with deployments in modern K8s, it's good to have experience with them.
To create the replication controller, write the following YAML file:

Replication Controller:

apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80


Write the YAML that will maintain three copies of an nginx container using a Replica Set.  Test it to be sure it works, then delete it.
A deployment is used to manage Replica Sets.
To maintain three copies of an nginx container in a replica set, write the following YAML file:

Replication Set:

apiVersion: apps/v1beta2
kind: ReplicaSet
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80

Write the YAML for a deployment that will maintain three copies of an nginx container.
  Test it to be sure it works, then delete it.
To perform a deployment for the Replica Set, write the following YAML file:

Deployment:

apiVersion: apps/v1beta2 # for versions before 1.8.0 use apps/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80

Labels & Selectors
Enable to map their own organizational structure onto Kube objects
Example labels
"release" : "stable", "release" : "canary"
"environment" : "dev", "environment" : "qa", "environment" : "production"
"partition" : "customerA", "partition" : "customerB"
"tier" : "backend", "tier" : "frontend", "tier" : "cache"
"track" : "daily", "track": "weekly"

Key must be unique for a given object

kubectl get pods -l app=nginx
kubectl get pods -l app=mysql
kubectl label pod mysql-544bbdcd6f-grftk test=sure
kubectl label pod mysql-544bbdcd6f-grftk test=sure --overwrite
kubectl describe pod -l test=sure
kubectl label pods -l app=nginx tier=frontend
kubectl delete pods -l test=sure


