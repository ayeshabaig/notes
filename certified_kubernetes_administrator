Certified Kubernetes Administrator

Set up your practice cluster
Playground
Ubuntu, North America, Medium, Kube Master, Kube Node 1, Kube Node 2

ssh into VMs

Add the Docker Repository on all three servers.
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository    "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"

Add the Kubernetes repository on all three servers.
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF

Install Docker, Kubeadm, Kubelet, and Kubectl on all three servers.
sudo apt-get update
sudo apt-get install -y docker-ce=18.06.1~ce~3-0~ubuntu kubelet=1.12.2-00 kubeadm=1.12.2-00 kubectl=1.12.2-00
sudo apt-mark hold docker-ce kubelet kubeadm kubectl

Enable net.bridge.bridge-nf-call-iptables on all three nodes.
echo "net.bridge.bridge-nf-call-iptables=1" | sudo tee -a /etc/sysctl.conf
sudo sysctl -p

On only the Kube Master server, initialize the cluster and configure kubectl.
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Install the flannel networking plugin in the cluster by running this command on the Kube Master server.
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml

The kubeadm init command that you ran on the master should output a kubeadm join command containing a token and hash. You will need to copy that command from the master and run it on both worker nodes with sudo.
sudo kubeadm join $controller_private_ip:6443 --token $token --discovery-token-ca-cert-hash $hash

Now you are ready to verify that the cluster is up and running. On the Kube Master server, check the list of nodes.
kubectl get nodes

It should look something like this:
NAME                      STATUS   ROLES    AGE   VERSION
wboyd1c.mylabserver.com   Ready    master   54m   v1.12.2
wboyd2c.mylabserver.com   Ready    <none>   49m   v1.12.2
wboyd3c.mylabserver.com   Ready    <none>   49m   v1.12.2

Make sure that all three of your nodes are listed and that all have a STATUS of Ready.

Architecture Diagram & Explanation
https://linuxacademy.com/cp/guides/download/refsheets/guides/refsheets/linuxacademy-kubernetesadmin-archdiagrams-1_1516737832.pdf
Master Node
  Kube-APISERVER
    ETCD
    Kube-Scheduler
    Cloud-Controller-Manager <-> Cloud
    Kube-Controller-Manager
Node
  Kubelet
  Kube Proxy
  Pod
    Container

How to install Kubernetes on Centos
Create 3 VMs as with Ubuntu install

Turn off swap on all servers.
sudo swapoff -a
sudo vi /etc/fstab
Look for the line in /etc/fstab that says /root/swap and add a # at the start of that line, so it looks like: #/root/swap. Then save the file.

Install and configure Docker.
sudo yum -y install docker
sudo systemctl enable docker
sudo systemctl start docker

Add the Kubernetes repo.
cat << EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

Turn off selinux.
sudo setenforce 0
sudo vi /etc/selinux/config

Change the line that says SELINUX=enforcing to SELINUX=permissive and save the file.

Install Kubernetes Components.
sudo yum install -y kubelet kubeadm kubectl
sudo systemctl enable kubelet
sudo systemctl start kubelet

Configure sysctl.
cat << EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sudo sysctl --system

Initialize the Kube Master. Do this only on the master node.
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Install flannel networking.
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml

The kubeadm init command that you ran on the master should output a kubeadm join command containing a token and hash. You will need to copy that command from the master and run it on all worker nodes with sudo.
sudo kubeadm join $controller_private_ip:6443 --token $token --discovery-token-ca-cert-hash $hash

Now you are ready to verify that the cluster is up and running. On the Kube Master server, check the list of nodes.
kubectl get nodes

It should look something like this:
NAME                      STATUS   ROLES    AGE     VERSION
wboyd4c.mylabserver.com   Ready    master   3m36s   v1.12.2
wboyd5c.mylabserver.com   Ready    <none>   23s     v1.12.2
Make sure that all of your nodes are listed and that all have a STATUS of Ready.

Kubernetes API Primitives (objects) & Cluster Architecture
API Primtives
- Persistent entities in the Kubernetes System.
- Uses these to represent the state of the cluster.
- Describe:
    What applications are running
    Which nodes those applications are running on.
    Policies around those applications.
- Kubernetes Objects are "records of intent"

Object Spec:
- Provided to Kubernetes
- Describes desired state of objects

Object Status:
- Provided by Kubernetes
- Describes the actual state of the object

Kubernetes Yaml
kubectl turns yaml into json
apiVersion: v1 -> # required
kind: Pod # what kind of object is being described
metadata: # data used to uniquely identify object (maybe name, uid, namespace
  name: busybox
spec: # format of spec field is different for every object, kube api reference has format rules
  containers:
  - name: busybox
    image: busybox
    command:
      - sleep
      - "3600"

Common Kube Objects
- Nodes # hosts that make up cluster
- Pods # single instances of application containers
- Deployments # sets of load-balanced pods
- Services # expose deployments to external networks
- ConfigMaps # key-value pairs that can be plugged into other objects as needed

Names and UIDs
Names
- All objects have a unique name.
- Client provided.
- Can be reused.
- Maximum length of 253 chars
- Lower case alphanumeric chars
- - and . allowed
UIDs
- All objects have a unique UID.
- Generated by Kubernetes.
- Spatially and temporally unique.

Namespaces
- Multiple virtual clusters backed by the same virtual cluster
- Generally for large deployments.
- Provide scope for names.
- Easy way to divide cluster resources.
- Allows for multiple teams of users.
- Allows for resource quotas.
- Special 'kube-system' namespace
    Used to differentiate system pods from user pods.

Nodes
- Might be a VM or physical machine.
- Services necessary to run pods.
- Managed by the master.
- Services necessary
    Container runtime
    Kubelet
    Kube-proxy
- Not inherently created by Kubernetes, but by Cloud Provide
- Kubernetes checks the node for validity.

Cloud Controller Managers
- Route controller (gce clusters only)
- Service Controller
- PersistentVolumeLabels controller

Node Controller
- Assigns CIDR block to a newly registered node.
- Keeps track of the nodes.
- Monitors the node health.
- Evicts pods from unhealthy nodes
- Can taint nodes based on current conditions in more recent versions.

Kubernetes Services
- Underlying architecture
- Pod -- Simplest kubernetes object, represents one or more containers running on a single node.
- Ephemeral, disposable, and replaceable - Stateless
- "Cattle vs. Pets"
- Usually managed via Deployments

- Deployment specifications - refer to details of pods like images and number of replicas

Services refer to Deployments and expose a particular port of IP address

Running the application pods.
  How you set up a service depends on your networking configuration
  and how you will handle load balancing and port forwarding.
  If you use the pod network IP address method, then a deployment gets
  assigned a single IP address -- even if there are multiple replicas of that pod.
  The kubernetes service (using kube-proxy on the node) redirects traffic.

Commands can be imperative or declarative
Imperative
  kubectl run nginx --image=nginx
Declarative
  apiVersion: apps/v1
  kind: Deployment
  metadata: ...

Points to remember:
- Containers are run in Pods, the simplest Kubernetes object representing an application.
- Pods are (usually) managed by deployments.
- Services expose deployments.
- Third parties handle load balancing or port forwarding to those services,
  though Ingress objects (along with an appropriate ingress controller) are needed to do that work.

sample yaml for a job which uses perl to calculate pi to 2000 digits and stops
---
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
  backoffLimit: 4

yaml for a job to create a busybox pod that sleeps for 10 seconds and stops
---
apiVersion: batch/v1
kind: Job
metadata:
  name: busybox
spec:
  template:
    spec:
      containers:
      - name: busybox
        image: busybox
        command: ["sleep", "10"]
      restartPolicy: Never
  backoffLimit: 4

kubectl describe job pi -> use that to get pod name and run
kubectl logs [pod-name]

This pod will cause the alpine linux container to sleep for 3600 seconds (1 hour) and then exit. Kubernetes will then restart the pod.
---
apiVersion: v1
kind: Pod
metadata:
  name: alpine
  namespace: default
spec:
  containers:
  - name: alpine
    image: alpine
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always

To create the pod: kubectl create -f alpine.yaml
To delete the pod: kubectl delete -f alpine.yaml
  or kubectl delete pod alpine or kubectl delete pod/alpine

yaml example for nginx pod:
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  -  name: nginx
      image: nginx
  restartPolicy: Always

Designing a Kubernetes Cluster
Use the kubernetes cluser on linux academy playground
minikube- single node on local- linux, windows, mac
kubeadm to deploy multinode locally, need your own CNI (flannel)
Ubuntu on LXD provides nine-instance supported kube cluster on local
cloud providers - google, azure, stackpoint, etc
turnkey solutions - up and running fast - google, aws, oracle
add-on solutions- most important is CNI Container Networking Interface
  Calico is a secure L3 networking and network policy provider
  Canal unites Flannel and Calico, providing networking and network policy
  Cilium is a L3 network and network policy plugin that can enforce HTTP/APL/L7 policies transparently
    Both routing and overlay/encapsulation mode are supported.
  CNI-Genie enables Kubernetes to seamlessly connect to a choice of CNI plugins,
    such as Calico, Canal, Flannel, Romana, or Weave
Contiv
  provides configurable networking (native L3 using BGP, overlay using vxlan, classic L2, and Cisco-SDN/ACI)
  for various uses and rich policy framework.
  fully open sourced
  installer provides both kubeadm and non-kubeadm based installation options
Flannel is an overlay network provider that can be used with Kubernetes, and is the one we're using in our LA servers
Multus is a Multi plugin for multiple network support in Kubernetes to support all CNI plugins
  (e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, DPDK, OVS-DPDK, and VPP based workloads in Kubernetes
NSX-T Container Plug-in (NCP) provides integration between VMWare NSX-T and container orchestrators such as Kube
  Also provides integrations between NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service PKS and Openshift
Nuage is an SDN platform that provide policy-based networking between Kubernetes Pods and
  non-Kube environments with visibility and security monitoring
Romana is a Layer 3 networking solution for pod networks that also supports the NetworkPolicy API
Weave Net provides networking and network policy, will carry on working on both sides of a network partition,
  and does not require an external database
There are also add-ons for service discovery and visualization and control, like the dashboard.

Exercise: Explore the Sandbox
1. Examine the current status of your cluster. Are all the nodes ready? How do you know?
The command kubectl get nodes will give the current status of all nodes.

2. Are there any pods running on node 2 of your cluster? How can you tell?
You can get this information in a variety of ways:
kubectl describe node node-name
kubectl get pods --all-namespaces -o wide will list all pods and which nodes they are currently running on.

3. Is the master node low on memory currently? How can you tell?
kubectl describe node node-2-name will list DiskPressure and MemoryPressure statuses so you can see how it is doing.

4. What pods are running in the kube-system namespace? What command did you use to find out?
kubectl get pods -n kube-system will provide the desired results.

Hardware and Underlying Infrastructure
The hardware and infrastructure that k8s will run on is truly staggering
Many developers run full, 6 node k8s and docker clusters on raspberry pis
Nodes, including the master, can be physical or virtual machines running k8s componenets
  and a container manager such as docker or rocket
The nodes should be connected by a common network, thought he internet will work as long as
  port 443 and whatever your pod networking system uses are unblocked
A pod networking application such as Flannel is needed to allow the pods to communicate
  with one another, and it makes use of an overlay network (by default it's vxlans)
  to provide that service
The Linux Academy Kubernetes Lab you built on Cloud Servers in an earlier lab, for example,
  is made up of two to six VMs. We then used kubeadm to bootstrap the master, and ran
  a join command on each of the nodes to provision them.
For the exam, it's important to understand this relationship.

Cluster Communications
- Cluster communications cover communications to the API server, control-plane communications inside the cluster,
  and can even include pod-to-pod communications.
- This is an in-depth topic with a fair amount of detail, so let's start by discussing how to secure
  communications to the Kubernetes API server.
- Everything in Kubernetes goes through the API driver, so controlling and limiting who has access
  to the cluster and what they are allowed to do is arguably the most important task in securing the cluster.
- The default encryption communication in K8s is TLS
- Most of the installation methods will handle the certificate creation.
- Kubeadm created certificates during the Linux Academy Cloud Server Kubernetes Cluster Lab
- No matter how you've installed k8s, some components and installation methods may enable
  local ports over HTTP and you should double check the settings of these components to
  identify potentially unsecured traffic and address these issues.
- Anything that connects to the API, including nodes, proxies, the scheduler, volume plugins in additions
  to users, should be authenticated.
- Again, most installation methods create certificates for those infrastructure pieces, but if you've chosen to install manually,
  you might need to do this yourself.
- Once authenticated, every API call should pass an authorization check.
RBAC
- Kubernetes has an integrated Role-Based Access Control (RBAC) component
- Certain roles perform specific actions in the cluster
- Kubernetes has several well thought out, pre-created roles
- Simple rules might be fine for smaller clusters
- If a user doesn't have rights to perform and action, but they do have access to perform a composite
  action that includes it, the user WILL be able to indirectly create objects.
- Carefully consider what you want users to be allowed to do prior to making changes to the existing roles.

Securing the Kubelet
- Secure the kubelet on each node.
- The Kubelets expose HTTPS endpoints which give access to both data and actions on the nodes.
  By default, these are open.
- To secure those endpoints, you can enable Kubelet Authentication and Authorization by starting it
  with an --anonymous-auth=false flag and assigning it an appropriate x509 client certificate in its configuration.

Securing the Network
- Network Policies restrict access to the network for a particular namespace.
- This allows developers to restrict which pods in other namespaces can access pods and port within
  the current namespace.
- The pod networking CNI must respect these policies which, fortunately, most of them do
- Users can also be assigned quotas or limit ranges
- Use Plug-Ins for more advanced functionality
- That should secure all the communications in a cluster

Vulnerabilities
- Kubernetes makes extensive use of etcd for storing configuration and secrets. It acts as the
  key/value store for the entire cluster.
- Gaining write access to etcd is very much like gaining root on the whole cluster, and even
  read access can be used by attackers to cause some serious damage.
- Strong credentials on your etcd server or cluster is a must.
- Isolate those behind a firewall that only allows requests from the API servers.

- Audit logging is also critical
- Records actions taken by the API for later analysis in the event
- Enable audit logging and archive the audit file on a secure server

- Rotate your infrastructure credentials frequently
- Smaller lifetime windows for secrets and credentials create bigger problems for attackers attempting to use it.
- You can even set these up to have short lifetimes and automate their rotation.

Third Party Integrations
- Always review third party integrations before enabling them.
- Integrations to Kubernetes can change how secure your cluster is.
- Add-ons might be nothing more than just more pods in the cluster, but those can be powerful.
- Don't allow them into the kube-system namespace

- You should also join the kubernetes-announce group for emails about security announcements
  https://groups.google.com/forum/#!forum/kubernetes-announce

Making Kubernetes Highly Available
- The challenges associated with making any platform HA are many
- While the exam isn't going to make you deploy a full HA kubernetes cluster, we should take a look
  at the process from a high level to gain an understanding of how it should work

The Process
- Create the reliable nodes that will form our cluster.
- Set up a redundant and reliable storage service with a multinode deployment of etcd
- Start replicated and load balanced Kubernetes API servers.
- Set up a master-elected kubernetes scheduler and controller manager daemons.
- Here is what our final system is going to look like
- Notice that everything that needs the API server or any other service on the master goes through
  the load balancer, including the worker nodes.

Step One
- Make the Master node reliable
- Ensure that the services automatically restart if they fail
- Kubelet already does this, so it's a convenient piece to use
- If the kubelet goes down, though, we need to restart it
- Monit on Debian systems or systemctl on systemd-based systems.

Step Two - Storage Layer
- If we're going to make it HA, then we've got to make sure that persistent storage is rock solid
- Protect the data
- If you have the data, you can rebuild almost anything else.
- Once the data is gone, the cluster is gone too
- Clustered etcd already replicates the storage to all master instances in your cluster
- To lose data, all three nodes would need to have their disks fail at the same time.
- The probability of this is relatively low, so just running a replicated etcd cluster is reliable enough.
- Add additional reliability by increasing the size of the cluster from three to five nodes.

