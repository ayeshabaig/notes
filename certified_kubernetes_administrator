Certified Kubernetes Administrator

Set up your practice cluster
Playground
Ubuntu, North America, Medium, Kube Master, Kube Node 1, Kube Node 2

ssh into VMs

Add the Docker Repository on all three servers.
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository    "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"

Add the Kubernetes repository on all three servers.
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF

Install Docker, Kubeadm, Kubelet, and Kubectl on all three servers.
sudo apt-get update
sudo apt-get install -y docker-ce=18.06.1~ce~3-0~ubuntu kubelet=1.12.2-00 kubeadm=1.12.2-00 kubectl=1.12.2-00
sudo apt-mark hold docker-ce kubelet kubeadm kubectl

Enable net.bridge.bridge-nf-call-iptables on all three nodes.
echo "net.bridge.bridge-nf-call-iptables=1" | sudo tee -a /etc/sysctl.conf
sudo sysctl -p

On only the Kube Master server, initialize the cluster and configure kubectl.
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Install the flannel networking plugin in the cluster by running this command on the Kube Master server.
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml

The kubeadm init command that you ran on the master should output a kubeadm join command containing a token and hash. You will need to copy that command from the master and run it on both worker nodes with sudo.
sudo kubeadm join $controller_private_ip:6443 --token $token --discovery-token-ca-cert-hash $hash

Now you are ready to verify that the cluster is up and running. On the Kube Master server, check the list of nodes.
kubectl get nodes

It should look something like this:
NAME                      STATUS   ROLES    AGE   VERSION
wboyd1c.mylabserver.com   Ready    master   54m   v1.12.2
wboyd2c.mylabserver.com   Ready    <none>   49m   v1.12.2
wboyd3c.mylabserver.com   Ready    <none>   49m   v1.12.2

Make sure that all three of your nodes are listed and that all have a STATUS of Ready.

Architecture Diagram & Explanation
https://linuxacademy.com/cp/guides/download/refsheets/guides/refsheets/linuxacademy-kubernetesadmin-archdiagrams-1_1516737832.pdf
Master Node
  Kube-APISERVER
    ETCD
    Kube-Scheduler
    Cloud-Controller-Manager <-> Cloud
    Kube-Controller-Manager
Node
  Kubelet
  Kube Proxy
  Pod
    Container

How to install Kubernetes on Centos
Create 3 VMs as with Ubuntu install

Turn off swap on all servers.
sudo swapoff -a
sudo vi /etc/fstab
Look for the line in /etc/fstab that says /root/swap and add a # at the start of that line, so it looks like: #/root/swap. Then save the file.

Install and configure Docker.
sudo yum -y install docker
sudo systemctl enable docker
sudo systemctl start docker

Add the Kubernetes repo.
cat << EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

Turn off selinux.
sudo setenforce 0
sudo vi /etc/selinux/config

Change the line that says SELINUX=enforcing to SELINUX=permissive and save the file.

Install Kubernetes Components.
sudo yum install -y kubelet kubeadm kubectl
sudo systemctl enable kubelet
sudo systemctl start kubelet

Configure sysctl.
cat << EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sudo sysctl --system

Initialize the Kube Master. Do this only on the master node.
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Install flannel networking.
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml

The kubeadm init command that you ran on the master should output a kubeadm join command containing a token and hash. You will need to copy that command from the master and run it on all worker nodes with sudo.
sudo kubeadm join $controller_private_ip:6443 --token $token --discovery-token-ca-cert-hash $hash

Now you are ready to verify that the cluster is up and running. On the Kube Master server, check the list of nodes.
kubectl get nodes

It should look something like this:
NAME                      STATUS   ROLES    AGE     VERSION
wboyd4c.mylabserver.com   Ready    master   3m36s   v1.12.2
wboyd5c.mylabserver.com   Ready    <none>   23s     v1.12.2
Make sure that all of your nodes are listed and that all have a STATUS of Ready.

Kubernetes API Primitives (objects) & Cluster Architecture
API Primtives
- Persistent entities in the Kubernetes System.
- Uses these to represent the state of the cluster.
- Describe:
    What applications are running
    Which nodes those applications are running on.
    Policies around those applications.
- Kubernetes Objects are "records of intent"

Object Spec:
- Provided to Kubernetes
- Describes desired state of objects

Object Status:
- Provided by Kubernetes
- Describes the actual state of the object

Kubernetes Yaml
kubectl turns yaml into json
apiVersion: v1 -> # required
kind: Pod # what kind of object is being described
metadata: # data used to uniquely identify object (maybe name, uid, namespace
  name: busybox
spec: # format of spec field is different for every object, kube api reference has format rules
  containers:
  - name: busybox
    image: busybox
    command:
      - sleep
      - "3600"

Common Kube Objects
- Nodes # hosts that make up cluster
- Pods # single instances of application containers
- Deployments # sets of load-balanced pods
- Services # expose deployments to external networks
- ConfigMaps # key-value pairs that can be plugged into other objects as needed

Names and UIDs
Names
- All objects have a unique name.
- Client provided.
- Can be reused.
- Maximum length of 253 chars
- Lower case alphanumeric chars
- - and . allowed
UIDs
- All objects have a unique UID.
- Generated by Kubernetes.
- Spatially and temporally unique.

Namespaces
- Multiple virtual clusters backed by the same virtual cluster
- Generally for large deployments.
- Provide scope for names.
- Easy way to divide cluster resources.
- Allows for multiple teams of users.
- Allows for resource quotas.
- Special 'kube-system' namespace
    Used to differentiate system pods from user pods.

Nodes
- Might be a VM or physical machine.
- Services necessary to run pods.
- Managed by the master.
- Services necessary
    Container runtime
    Kubelet
    Kube-proxy
- Not inherently created by Kubernetes, but by Cloud Provide
- Kubernetes checks the node for validity.

Cloud Controller Managers
- Route controller (gce clusters only)
- Service Controller
- PersistentVolumeLabels controller

Node Controller
- Assigns CIDR block to a newly registered node.
- Keeps track of the nodes.
- Monitors the node health.
- Evicts pods from unhealthy nodes
- Can taint nodes based on current conditions in more recent versions.

Kubernetes Services
- Underlying architecture
- Pod -- Simplest kubernetes object, represents one or more containers running on a single node.
- Ephemeral, disposable, and replaceable - Stateless
- "Cattle vs. Pets"
- Usually managed via Deployments

- Deployment specifications - refer to details of pods like images and number of replicas

Services refer to Deployments and expose a particular port of IP address

Running the application pods.
  How you set up a service depends on your networking configuration
  and how you will handle load balancing and port forwarding.
  If you use the pod network IP address method, then a deployment gets
  assigned a single IP address -- even if there are multiple replicas of that pod.
  The kubernetes service (using kube-proxy on the node) redirects traffic.

Commands can be imperative or declarative
Imperative
  kubectl run nginx --image=nginx
Declarative
  apiVersion: apps/v1
  kind: Deployment
  metadata: ...

Points to remember:
- Containers are run in Pods, the simplest Kubernetes object representing an application.
- Pods are (usually) managed by deployments.
- Services expose deployments.
- Third parties handle load balancing or port forwarding to those services,
  though Ingress objects (along with an appropriate ingress controller) are needed to do that work.

sample yaml for a job which uses perl to calculate pi to 2000 digits and stops
---
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
  backoffLimit: 4

yaml for a job to create a busybox pod that sleeps for 10 seconds and stops
---
apiVersion: batch/v1
kind: Job
metadata:
  name: busybox
spec:
  template:
    spec:
      containers:
      - name: busybox
        image: busybox
        command: ["sleep", "10"]
      restartPolicy: Never
  backoffLimit: 4

kubectl describe job pi -> use that to get pod name and run
kubectl logs [pod-name]

This pod will cause the alpine linux container to sleep for 3600 seconds (1 hour) and then exit. Kubernetes will then restart the pod.
---
apiVersion: v1
kind: Pod
metadata:
  name: alpine
  namespace: default
spec:
  containers:
  - name: alpine
    image: alpine
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always

To create the pod: kubectl create -f alpine.yaml
To delete the pod: kubectl delete -f alpine.yaml
  or kubectl delete pod alpine or kubectl delete pod/alpine

yaml example for nginx pod:
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  -  name: nginx
      image: nginx
  restartPolicy: Always

Designing a Kubernetes Cluster
Use the kubernetes cluser on linux academy playground
minikube- single node on local- linux, windows, mac
kubeadm to deploy multinode locally, need your own CNI (flannel)
Ubuntu on LXD provides nine-instance supported kube cluster on local
cloud providers - google, azure, stackpoint, etc
turnkey solutions - up and running fast - google, aws, oracle
add-on solutions- most important is CNI Container Networking Interface
  Calico is a secure L3 networking and network policy provider
  Canal unites Flannel and Calico, providing networking and network policy
  Cilium is a L3 network and network policy plugin that can enforce HTTP/APL/L7 policies transparently
    Both routing and overlay/encapsulation mode are supported.
  CNI-Genie enables Kubernetes to seamlessly connect to a choice of CNI plugins,
    such as Calico, Canal, Flannel, Romana, or Weave
Contiv
  provides configurable networking (native L3 using BGP, overlay using vxlan, classic L2, and Cisco-SDN/ACI)
  for various uses and rich policy framework.
  fully open sourced
  installer provides both kubeadm and non-kubeadm based installation options
Flannel is an overlay network provider that can be used with Kubernetes, and is the one we're using in our LA servers
Multus is a Multi plugin for multiple network support in Kubernetes to support all CNI plugins
  (e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, DPDK, OVS-DPDK, and VPP based workloads in Kubernetes
NSX-T Container Plug-in (NCP) provides integration between VMWare NSX-T and container orchestrators such as Kube
  Also provides integrations between NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service PKS and Openshift
Nuage is an SDN platform that provide policy-based networking between Kubernetes Pods and
  non-Kube environments with visibility and security monitoring
Romana is a Layer 3 networking solution for pod networks that also supports the NetworkPolicy API
Weave Net provides networking and network policy, will carry on working on both sides of a network partition,
  and does not require an external database
There are also add-ons for service discovery and visualization and control, like the dashboard.

Exercise: Explore the Sandbox
1. Examine the current status of your cluster. Are all the nodes ready? How do you know?
The command kubectl get nodes will give the current status of all nodes.

2. Are there any pods running on node 2 of your cluster? How can you tell?
You can get this information in a variety of ways:
kubectl describe node node-name
kubectl get pods --all-namespaces -o wide will list all pods and which nodes they are currently running on.

3. Is the master node low on memory currently? How can you tell?
kubectl describe node node-2-name will list DiskPressure and MemoryPressure statuses so you can see how it is doing.

4. What pods are running in the kube-system namespace? What command did you use to find out?
kubectl get pods -n kube-system will provide the desired results.

Hardware and Underlying Infrastructure
The hardware and infrastructure that k8s will run on is truly staggering
Many developers run full, 6 node k8s and docker clusters on raspberry pis
Nodes, including the master, can be physical or virtual machines running k8s componenets
  and a container manager such as docker or rocket
The nodes should be connected by a common network, thought he internet will work as long as
  port 443 and whatever your pod networking system uses are unblocked
A pod networking application such as Flannel is needed to allow the pods to communicate
  with one another, and it makes use of an overlay network (by default it's vxlans)
  to provide that service
The Linux Academy Kubernetes Lab you built on Cloud Servers in an earlier lab, for example,
  is made up of two to six VMs. We then used kubeadm to bootstrap the master, and ran
  a join command on each of the nodes to provision them.
For the exam, it's important to understand this relationship.

Cluster Communications
- Cluster communications cover communications to the API server, control-plane communications inside the cluster,
  and can even include pod-to-pod communications.
- This is an in-depth topic with a fair amount of detail, so let's start by discussing how to secure
  communications to the Kubernetes API server.
- Everything in Kubernetes goes through the API driver, so controlling and limiting who has access
  to the cluster and what they are allowed to do is arguably the most important task in securing the cluster.
- The default encryption communication in K8s is TLS
- Most of the installation methods will handle the certificate creation.
- Kubeadm created certificates during the Linux Academy Cloud Server Kubernetes Cluster Lab
- No matter how you've installed k8s, some components and installation methods may enable
  local ports over HTTP and you should double check the settings of these components to
  identify potentially unsecured traffic and address these issues.
- Anything that connects to the API, including nodes, proxies, the scheduler, volume plugins in additions
  to users, should be authenticated.
- Again, most installation methods create certificates for those infrastructure pieces, but if you've chosen to install manually,
  you might need to do this yourself.
- Once authenticated, every API call should pass an authorization check.
RBAC
- Kubernetes has an integrated Role-Based Access Control (RBAC) component
- Certain roles perform specific actions in the cluster
- Kubernetes has several well thought out, pre-created roles
- Simple rules might be fine for smaller clusters

- If a user doesn't have rights to perform and action, but they do have access to perform a composite
  action that includes it, the user WILL be able to indirectly create objects.
- Carefully consider what you want users to be allowed to do prior to making changes to the existing roles.

Securing the Kubelet
- Secure the kubelet on each node.
- The Kubelets expose HTTPS endpoints which give access to both data and actions on the nodes.
  By default, these are open.
- To secure those endpoints, you can enable Kubelet Authentication and Authorization by starting it
  with an --anonymous-auth=false flag and assigning it an appropriate x509 client certificate in its configuration.

Securing the Network
- Network Policies restrict access to the network for a particular namespace.
- This allows developers to restrict which pods in other namespaces can access pods and port within
  the current namespace.
- The pod networking CNI must respect these policies which, fortunately, most of them do
- Users can also be assigned quotas or limit ranges
- Use Plug-Ins for more advanced functionality
- That should secure all the communications in a cluster

Vulnerabilities
- Kubernetes makes extensive use of etcd for storing configuration and secrets. It acts as the
  key/value store for the entire cluster.
- Gaining write access to etcd is very much like gaining root on the whole cluster, and even
  read access can be used by attackers to cause some serious damage.
- Strong credentials on your etcd server or cluster is a must.
- Isolate those behind a firewall that only allows requests from the API servers.

- Audit logging is also critical
- Records actions taken by the API for later analysis in the event
- Enable audit logging and archive the audit file on a secure server

- Rotate your infrastructure credentials frequently
- Smaller lifetime windows for secrets and credentials create bigger problems for attackers attempting to use it.
- You can even set these up to have short lifetimes and automate their rotation.

Third Party Integrations
- Always review third party integrations before enabling them.
- Integrations to Kubernetes can change how secure your cluster is.
- Add-ons might be nothing more than just more pods in the cluster, but those can be powerful.
- Don't allow them into the kube-system namespace

- You should also join the kubernetes-announce group for emails about security announcements
  https://groups.google.com/forum/#!forum/kubernetes-announce

Making Kubernetes Highly Available
- The challenges associated with making any platform HA are many
- While the exam isn't going to make you deploy a full HA kubernetes cluster, we should take a look
  at the process from a high level to gain an understanding of how it should work

The Process
- Create the reliable nodes that will form our cluster.
- Set up a redundant and reliable storage service with a multinode deployment of etcd
- Start replicated and load balanced Kubernetes API servers.
- Set up a master-elected kubernetes scheduler and controller manager daemons.
- Here is what our final system is going to look like
- Notice that everything that needs the API server or any other service on the master goes through
  the load balancer, including the worker nodes.

Step One
- Make the Master node reliable
- Ensure that the services automatically restart if they fail
- Kubelet already does this, so it's a convenient piece to use
- If the kubelet goes down, though, we need to restart it
- Monit on Debian systems or systemctl on systemd-based systems.

Step Two - Storage Layer
- If we're going to make it HA, then we've got to make sure that persistent storage is rock solid
- Protect the data
- If you have the data, you can rebuild almost anything else.
- Once the data is gone, the cluster is gone too
- Clustered etcd already replicates the storage to all master instances in your cluster
- To lose data, all three nodes would need to have their disks fail at the same time.
- The probability of this is relatively low, so just running a replicated etcd cluster is reliable enough.
- Add additional reliability by increasing the size of the cluster from three to five nodes.
- A multinode cluster of etcd
- If you use a cloud provider, then they usually provide this for you.
- If you are running on physical machines, you can also use network attached redundant storage using
  an iSCSI or NFS. Alternatively, you can run a clustered file system like Gluster or Ceph.
- RAID array on each physical machine

Step Three - Replicated API Services
- Create the initial log file so that Docker will mount a file instead of a directory:
  touch /var/log/kube-apiserver.log
- Next, we create a /srv/kubernetes directory on each node which should include:
  basic_auth.csv - basic auth user and password
  ca.crt - Certificate Authority cert
  known_tokens.csv - tokens that entities (e.g. the kubelet) can use to talk to the apiserver
  kubecfg.crt - client certificate, public key
  kubecfg.key - client certificate, private key
  server.cert - server certificate, public key
  server.key - server certificate, private key
- Either create this manually or copy it from a master node on a working cluster.
- Once these files exist, copy the kube-apiserver.yaml into /etc/kubernetes/manifests/ on each of our master nodes
- the kubelet monitors this directory, and will automatically create an instance of the
  kube-apiserver container using the pod definition specified in the file.
- If a network load balancer is set up, then access the cluster using the VIP and see traffic
  balancing between the apiserver instances.
- Setting up a load balancer will depend on the specifics of your platform.
- For external users of the API (e.g., the kubectl command line interface, continuous build pipelines, or other clients)
  remember to configure them to talk to the external load balancer's IP address.

Step Four - Controller/Scheduler Daemons
- Now we need to allow our state to change.
- Controller managers and schedulers.
- These processes must not modify the cluster's state simultaneously, use a lease-lock
- Each scheduler and controller manager can be launched with a --leader-elect flag
- The scheduler and controller-manager can be configured to talk to the API server that is on the
  same node (i.e. 127.0.0.1)
    It can also be configured to communicate using the load balanced IP address of the API servers.
- The scheduler and controller-manager will complete the leader election process mentioned before when
  using the --leader-elect flag
- In case of a failure accessing the API server, the elected leader will not be able to renew the lease,
  causing a new leader to be elected.
- This is especially relevant when configuring the scheduler and controller-manager to access the API server
  via 127.0.0.1, and the API server on the same node is unavailable
Installing Configuration Files
- Create empty log files on each node, so that Docker will mount the files and not make new directories:
  touch /var/log/kube-scheduler.log
  touch /var/log/kube-controller-manager.log
- Set up the descriptions of the scheduler and controller manager pods on each node by copying kube-scheduler.yaml
  and kube-controller-manager.yaml into the /etc/kubernetes/manifests/ directory
- And once that's all done, we've got our cluster highly available! Remember, if a worker node goes down,
  kubernetes will rapidly detect that and spin up replacement pods elsewhere in the cluster.

End-to-End Testing and Validation
- Provide a mechanism to test end-to-end behavior of the system
- Last signal to ensure end user operations match specifications
- Primarily a developer tool
- Difficult to run against just "any" deployment - many specific tests for cloud providers
    Ubuntu has its own Juju-deployed tests
    GCE has its own
    Many tests offered
Kubetest Suite
- Ideal for GCE or AWS users
- Build
- Stage
- Extract
- Bring up the cluster
- Test
- Dump logs
- Tear down
Conclusion
- e2e validation is primarily for developers creating new code
- Kubetest is a testing suite for Kubernetes e2e testing

Validating Nodes and the Cluster
Login to Kube Master
kubectl get nodes
kubectl describe node [server-name]
can also log into a worker node and run `ps aux | grep kube` to make sure
  kubelet, kube-proxy, and networking plugin are running

Deployments, Rolling Updates, and Rollbacks
nginx-deployment.yaml
---
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template: # describes the pods that are going to be created
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80

kubectl create -f nginx-deployment.yaml
kubectl get deployments
kubectl describe deployment nginx-deployment
kubectl get deployment nginx-deployment -o yaml -> way to get yaml back out

How to do a rollout
kubectl set image deployment nginx-deployment nginx=nginx:1.8
kubectl rollout status deployment/nginx-deployment
kubectl describe deployment nginx-deployment
vim nginx-deployment -> edit version to 1.9
kubectl apply -f nginx-deployment.yaml
kubectl rollout status deployment/nginx-deployment
kubectl describe deployment nginx-deployment
kubectl rollout history deployment/nginx-deployment --revision=3
kubectl rollout undo deployment/nginx-deployment --to-revision=2

Exercise: Deployments
1. Create the deployment.
  Create the yaml file and name it something. I chose nginx-deployment.yaml. Create the deployment object by calling:
  kubectl create -f nginx-deployment.yaml

2. Which nodes are the pods running on. How can you tell?
  There are many ways to get this, here's one example that gives you the results in one step and uses a label selector:
  kubectl get pods -l app=nginx -o wide

3. Update the deployment to use the 1.8 version of the nginx container and roll it out.
  There are many ways. Here are two:
  This will work just fine but is not the preferredmethod because now the yaml is inconsistent with what you've got running in the cluster. Anyonecomingacrossyouryaml will assume it's what is up and running and it isn't.
  kubectl set image deployment nginx-deployment nginx=nginx:1.8
  Updatethelineintheyaml to the 1.8 version of the image, and apply the changes with
  kubectl apply -f nginx-deployment.yaml

4. Update the deployment to use the 1.9.1 version of the nginx container and roll it out.
  Same as above. Don't forget you can watch the status of the rollout with the command

5. Roll back the deployment to the 1.8 version of the container.
  kubectl rollout status deployment nginx-deployment
  kubectl rollout undo deployment nginx-deployment
  will undo the previous rollout, or if you want to go to a specific point in history, you can view the history and roll back to a specific state with:
  kubectl rollout history deployment nginx-deployment
  kubectl rollout undo deployment nginx-deployment --to-revision=x

6. Remove the deployment
  kubectl delete -f nginx-deployment.yaml

How Kubernetes Configures Applications
ConfigMaps
kubectl create configmag my-map --from-literal=LinuxAcademy
kubectl get configmaps
kubectl describe configmaps my-map
kubectl get configmap my-map -o yaml
cat pod-config.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: config-test-pod
spec:
  containers:
    - name: test-container
      image: busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        - name: WHAT_SCHOOL
          valueFrom:
            configMapKeyRef:
              name: my-map
              key: school
  restartPolicy: never

kubectl create -f pod-config.yaml
kubectl logs config-test-pod

Exercise: Using ConfigMaps to Set Environment Variables
1. Write yaml for a ConfigMap that will create a data key called "myKey" with the value "myValue"
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config-map
data:
  myKey: myValue

2. Create the ConfigMap using the kubectl command-line tool
kubectl apply -f my-config-map.yaml to create the ConfigMap

3. Write a pod yaml to pass the ConfigMap data to the pod, so that the pod can use it as an environment variable
Here is a yaml file named "my-configmap-pod.yaml" that will pass the ConfigMap data to the pod so the container can use it as an environment variable:

apiVersion: v1
kind: Pod
metadata:
  name: my-configmap-pod
spec:
  containers:
  - name: myapp-container
    image: busybox
    command: ['sh', '-c', "echo $(MY_VAR) && sleep 3600"]
    env:
    - name: MY_VAR
      valueFrom:
        configMapKeyRef:
          name: my-config-map
          key: myKey

4. Create the pod using kubectl
kubectl apply -f my-configmap-pod.yaml to create a pod that will use the ConfigMap data as an environment variable

5. Echo the value of the environment variable to the console
kubectl logs my-configmap-pod to display the "echo $(MY_VAR)" command from the container

Scaling Applications
got a deployment with 2 replicas
to scale it to 3 replicas:
kubectl scale deployment/nginx-deployment --replicas=3
kubectl get deployments
kubectl get pods
kubectl scale deployment/nginx-deployment --replicas=1 -> to scale down
Autoscaling is possible with metrics and heapster, but not part of the exam

Scaling practice
---
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80

Scale the deployment up to 4 pods without editing the YAML.
kubectl scale deployment/nginx-deployment --replicas=4

Edit the YAML so that 3 pods are available and can apply the changes to the existing deployment.
edit the replicas line to 3 from 2 and
kubectl apply -f nginx-deployment.yaml

Which of these methods do you think is preferred and why?
  Performing the edit in the YAML is the preferred one,
  as it keeps the YAML on disk in sync with the state of the cluster.

Self-Healing Applications
all apps in kube are self-healing as kube changes things to match specs
kubectl get deployments
kubectl get pods
kubectl delete pod nginx-deployment-8989234
kubectl get pods -> a new pod is getting created
what if a node dies
shut off a node
kubectl get deployments -> shows 2 of 3 available
kubectl get pods -> still shows 3 pods, kube takes 5 mins to evict a pod it cant find
kubectl describe pod nginx-deployment-8989234 -> shows Tolerations: node-kubernetes.io/not-ready:NoExecute for 300s
kubectl get pods -> after 300s shows Status Unknown for the pod in node that was stopped
power back up the node -> shows ready -> no new pods started up, still at 3 replicas

Exercise: Replication Controllers, Replica Sets, and Deployments
Write a YAML file that will create a Replication Controller that will maintain three copies of an nginx container.
  Execute your YAML and make sure it works.
A Replica Set is a more advanced version of a Replication Controller that is used when more low-level control is needed.
  While these are commonly managed with deployments in modern K8s, it's good to have experience with them.
To create the replication controller, write the following YAML file:

Replication Controller:

apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80


Write the YAML that will maintain three copies of an nginx container using a Replica Set.  Test it to be sure it works, then delete it.
A deployment is used to manage Replica Sets.
To maintain three copies of an nginx container in a replica set, write the following YAML file:

Replication Set:

apiVersion: apps/v1beta2
kind: ReplicaSet
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80

Write the YAML for a deployment that will maintain three copies of an nginx container.
  Test it to be sure it works, then delete it.
To perform a deployment for the Replica Set, write the following YAML file:

Deployment:

apiVersion: apps/v1beta2 # for versions before 1.8.0 use apps/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80

Labels & Selectors
Enable to map their own organizational structure onto Kube objects
Example labels
"release" : "stable", "release" : "canary"
"environment" : "dev", "environment" : "qa", "environment" : "production"
"partition" : "customerA", "partition" : "customerB"
"tier" : "backend", "tier" : "frontend", "tier" : "cache"
"track" : "daily", "track": "weekly"

Key must be unique for a given object

kubectl get pods -l app=nginx
kubectl get pods -l app=mysql
kubectl label pod mysql-544bbdcd6f-grftk test=sure
kubectl label pod mysql-544bbdcd6f-grftk test=sure --overwrite
kubectl describe pod -l test=sure
kubectl label pods -l app=nginx tier=frontend
kubectl delete pods -l test=sure

Exercise: Label ALL THE THINGS!
1. Label each of your nodes with a "color" tag. The master should be black; node 2 should be red; node 3 should be green and node 4 should be blue.
  kubectl label node [kube-master-name] color=black
  kubectl label node [node1-name] color=red
  kubectl label node [node2-name] color=green
  kubectl label node [node3-name] color=blue

2. If you have pods already running in your cluster in the default namespace, label them with the key/value pair running=beforeLabels.
  kubectl label pods -n default running=beforeLabels --all

3. Create a new alpine deployment that sleeps for one minute and is then restarted from a yaml file that you write that labels these container with the key/value pair running=afterLabels.
  name: alpine
  namespace: default
  labels:
    running: afterLabels
spec:
  containers:
  - name: alpine
    image: alpine
    command:
      - sleep
      - "60"
  restartPolicy: Always

4. List all running pods in the default namespace that have the key/value pair running=beforeLabels.
  kubectl get pods -l running=beforeLabels -n default

5. Label all pods in the default namespace with the key/value pair tier=linuxAcademyCloud.
  kubectl label pods --all -n default tier=linuxAcademyCloud

6. List all pods in the default namespace with the key/value pair running=afterLabels and tier=linuxAcademyCloud.
  kubectl get pods -l running=afterLabels,tier=linuxAcademyCloud

DaemonSets
kubectl get daemonsets -n kube-system
kube-flannel-ds and kube-proxy
run everytime we require a pod to be on every node in cluster
kubectl describe daemonset kube-flannel-ds -n kube-system
used for networking, monitoring, etc

Exercise: Raise a DaemonSet
Write the yaml to deploy a DaemonSet (just use an nginx image) and
  then test it to be sure it gets deployed on each node.
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: cthulu
  labels:
    daemon: "yup"
spec:
  selector:
    matchLabels:
      daemon: "pod"
  template:
    metadata:
      labels:
        daemon: pod
  spec:
    tolerations:
    - key: node-role.kubernetes.io/master
      effect: NoSchedule
  containers:
  - name: cthulu-jr
    image: nginx

Resource Limits & Pod Scheduling
if scheduler cant find node with enough resources for a pod,
  the pod will remain in pending state until resources become available
taints allow nodes to repel work
kubectl get nodes
kubectl describe node [master-node-name]
shows the value
Taints: node-role.kubernetes.io/master:NoSchedule
Taints have a key, value, and an effect, represented as
  <key>=<value>:<effect>. The key 'node-role.kubernetes.io/master'
  is a null value and is not shown here in the taint.
NoSchedule means it wont schedule a node without a tolerance
tolerations make it possible for a pod to be scheduled to a node with a taint
You can remove taints by key, key-value, or key-effect. Here, the taint is removed by key
kubectl taint node [master-node-name] master-
To retaint the node:
The null value cannot be reapplied, but the taint 'node-role.kubernetes.io=master:NoSchedule'
  has the same effect
kubectl taint nodes [master-node-name] node-role.kubernetes.io/master-
If a toleration and a taint match during scheduling, the taint is ignored
  and the pod might be scheduled to the node.
Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints. Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints.

Manually Scheduling a Pod
To schedule to a particular node, gonna need labels
kubectl get nodes
if i want a pod on node 3, put a label on it
kubectl label node [node3-name] net=gigabit
kubectl describe node [node3-name] -> look in Labels section for new label
pod needs node selector on it for this node
do that in the yaml
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webhead
spec:
  replicas: 1
  selector:
    matchLabels:
      run: webhead
  template:
    metadata:
      labels:
        run: webhead
    spec:
      containers:
      - image: nginx
        name: webhead
        ports:
        - containerPort: 80
          protocol: TCP
      nodeSelector:
          net: gigabit

kubectl create -f webhead.yaml
kubectl get pods
kubectl describe pods -> can see it is on node 3

Exercise: Label a Node and Schedule a Pod
1. Pretend that node 3 is your favorite node.  Maybe it's got all SSDs.  Maybe it's got a fast network or a GPU.  Or maybe it sent you a nice tweet.  Label this node in some way so that you can schedule a pod to it.
  kubectl label node [node3-name] node=awesome

2. Create a yaml for a busybox sleeper/restarter that will get scheduled to your favorite node from #1.
---
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox
    name: busybox
    command:
      - "sleep"
      - "300"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always
  nodeSelector:
    node: awesome

Exercise: Multiple Schedulers


Pods generally are scheduled by the default scheduler, and their yaml might look like this:

apiVersion: v1
kind: Pod
metadata:
  name: annotation-default-scheduler
  labels:
    name: multischeduler
spec:
  schedulerName: default-scheduler
  containers:
  - name: pod-container
    image: k8s.gcr.io/pause:2.0

Ordinarily, we don't need to specify the scheduler's name in the spec because everyone uses a single default one. Sometimes, however, developers need to have custom schedulers in charge of placing pods due to legacy or specialized hardware constraints.

Rewrite the yaml for this pod so it makes use of a scheduler called custom-scheduler, and annotate the pod accordingly.

apiVersion: v1
kind: Pod
metadata:
  name: annotation-default-scheduler
  labels:
    name: multischeduler
  annotations:
    scheduledBy: custom-scheduler
spec:
  schedulerName: custom-scheduler
  containers:
  - name: pod-container
    image: k8s.gcr.io/pause:2.0

Monitoring Cluster and Application Components
- Monitoring is essential for a production system!
- Must monitor nodes, containers, pods, services, and the entire cluster.
- Provide end-users resource usage information

Heapster - Cluster-wide aggregator of monitoring and event data
Note: Support for installing Heapster has been removed from the Kubernetes setup
  scripts as of 1.12.0
Kubelet/cAdvisor on the Node
  Many nodes make up the cluster
+
Heapster runs on single node with Kubelet/Cadvisor
  Communicates with master
->
Storage Backend
Stores data from Heapster
  InfluxDB with Grafana
  Google Cloud Monitoring

cAdvisor is an open source container resource usage and performance analysis agent.
- Auto-discovers all containers on a node and collects CPU, memory, file system, and network usage stats.
- Provide the overall machine usage by analyzing the 'root' container on the machine.
- Exposes a simple UI for local containers on port 4194.

The Kubelet acts as a bridge between the Kubernetes master and the nodes.
Manages the pods and containers running on a node.
Translates each pod into the container making it up.
Obtains usage statistics from cAdvisor.
Exposes the aggregated pod resource usage statistics via a REST API

Grafana with InfluxDB
Heapster is setup to use this storage backend by default on most Kubernetes clusters.
InfluxDB and Grafana run in Pods.
The pod exposes itself as a Kubernetes service which is how Heapster then discovers it.
The Grafana container serves Grafana's UI which provides a dashboard.
The default dashboard for Kubernetes contains an example dashboard that monitors resource
  usage of the cluster and the pods inside it. This dashboard can, of course, be fully customized and expanded.
Google Cloud Monitoring is a hosted monitoring service that allows you to visualize
  and alert important metrics in your application
Heapster can be set up to automatically push all collected metrics to Google Cloud Monitoring
These metrics are then available in the Cloud Monitoring Console
This storage backend is the easiest to setup and maintain
The monitoring console allows you to easily create and customize dashboards using the exposed data

Managing Logs
kubectl get pods
kubectl logs [pod-name] -> shows stdout
kubectl get pods --all-namespaces
logs for most kube system pods are stored in the node at /var/log/containers
To check old logs with 'kubectl logs' use the --previous flag
can use stackdriver or fluentd to send all those logs to one place

Exercise: View the Logs
Create this object in your cluster:

apiVersion: v1
kind: Pod
metadata:
  name: counter
  labels:
    demo: logger
spec:
  containers:
  - name: count
    image: busybox
    args: [/bin/sh, -c, 'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 3; done']

This is a small container which wakes up every three seconds and logs the date and a count to stdout.

1. View the current logs of the counter.
  kubectl logs counter

2. Allow the container to run for a few minutes while viewing the log interactively.
  kubectl logs counter -f

3. Have the command only print the last 10 lines of the log.
  kubectl logs counter --tail=10 (--tail defaults to 10, so the 10 can be omitted in this case)

4. Look at the logs for the scheduler. Have there been any problems lately that you can see?
  This question really wants to know if you can find the logs for the scheduler.
  They're in the master in the /var/log/containers directory.
  There, a symlink has been create to the appropriate container's log file,
  and the symlink will have a name that begins with "kube-scheduler-"
  These logs belong to root, so you will have to sudo to view them.

5. Kubernetes uses etcd for its key-value store. Take a look at its logs and see if it has had any problems lately.
  The etcd logs are in the same directory as the logs for the previous question,
  only the name of the symlink begins with "etcd-", and also belongs to root.

6. Kubernetes API server also runs as a pod in the cluster. Find and examine its logs.
  The API server also lives in the same directory and begins with "kube-apiserver-", and also belongs to root.

I'm troubleshooting an application issue and would love to see the application's logs,
  which are in a file in the container "appctn" in the pod "apppod-abcdef123-45678" at /var/log/applog.
  Which of these commands would list that for me?
kubectl exec apppod-abcdef123-45678 -- cat /var/log/applog
kubectl logs only work for STDOUT, so if your logs are elsewhere, you'll need to pull them with something like the command here.

kubectl top, along with the object you'd like to watch, gives some in-depth information right on the command line. Who needs a GUI?
kubectl top [nodes | pods]

Is it possible to get a shell prompt to a Ubuntu 16.04 based container called "sidecar1" in the pod "star-aaaaaaaaaa-bbbbb"?
There are several containers in the pod. If so, how?
Yes! kubectl exec -it star-aaaaaaaaaa-bbbbb --container sidecar1 -- /bin/bash

Upgrading Kubernetes Components
without taking down the cluster
kubectl get nodes -> shows version
upgrade control plane
sudo apt upgrade kubeadm
kubeadm version
sudo kubeadm upgrade plan
sudo kubeadm upgrade apply v1.9.1
upgrade CNI, if possible
kubectl get deployment
kubectl get pods -o wide -> see nodes
kubectl drain [node-name] ignore-daemonsets
update kubelet
sudo apt update
sudo apt upgrade kubelet
systemctl status kubelet
kubectl get nodes -> master in status Ready, SchedulingDisabled
kubectl uncordon [master-node-name]
kubectl get nodes
moving on to second node, need to evict pod to run somewhere else
kubectl drain [node2-name] --ignore-daemonsets
kubectl get nodes
ssh into node2
sudo apt update
sudo apt upgrade kubelet
kubectl get nodes
kubectl uncordon [node2-name]

Upgrading the Underlying Operating System
Let's say we need to reinstall OS on a node, here's how
kubectl get nodes -> we're gonna do node4
kubectl drain [node4-name] --ignore-daemonsets -> cordons node
turn off node
kubectl get nodes -> eventually node shows as NotReady
kubectl delete node [node4-name] -> might as well delete it, its not coming back
to put a new node back on
kubeadm join comman is not around and the token is probably expired
sudo kubeadm token list -> check out tokens
sudo kubeadm token generate -> make a new token
sudo kubeadm create [token-from-command-above] --ttl 3h --print-join-command
  -> generates new kubeadm join command
ssh into new node and run kubeadm join command from above

1. Prepare node 3 for maintenance by preventing the scheduler from putting new pods on to it and evicting any existing pods.
  Ignore the DaemonSets -- those pods are only providing services to other local pods and will come back up when the node comes back up.
kubectl drain [node3-name] --ignore-daemonsets

2. When you think you've done everything correctly, go to the Cloud Servers page and shut node 3 down.
  Don't delete it! Just stop it. While it's down, we'll pretend that it's getting that new multiverse battery.
  While you wait for the cluster to stabilize, practice your yaml writing skills by creating yaml for a new deployment
  that will run six replicas of an image called k8s.gcr.io/pause:2.0. Name this deployment "lots-of-nothing".
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lots-of-nothing
spec:
  selector:
    matchLabels:
      timeToGet: schwifty
  replicas: 6
  template:
    metadata:
      labels:
        timeToGet: schwifty
    spec:
      containers:
      - name: pickle-rick
        image: k8s.gcr.io/pause:2.0

3. Bring the "lots-of-nothing" deployment up on your currently 75% active cluster. Notice where the pods land.
  kubectl create -f nothing-deployment.yaml
  kubectl get nodes
  kubectl get pods -o wide -l timeToGet=schwifty

4. Imagine you just got a text message from the server maintenance crew saying that the installation is complete.
  Go back to the Cloud Server tab and start Node 3 up again. Fiddle with your phone and send someone a text message
  if it helps with the realism.  Once Node 3 is back up and running and showing a "Ready" status, allow the scheduler to use it again.
  kubectl uncordon [node3-name]

5. Did any of your pods move to take advantage of the additional power?
  No, the uncordon only affects pods being scheduled and won't move any back unless other nodes are experiencing MemoryPressure or DiskPressure.

Node Networking Configuration
Inbound Node Port Requirements
Master Node(s)
TCP 6443 - Kubernetes API Server
TCP 2379-2380 - etcd server client API
TCP 10250 - Kubelet API
TCP 10251 - kube-scheduler
TCP 10252 - kube-controller-manager
TCP 10256 - Read-only Kubernetes API
Worker Nodes
TCP 10250 - Kubelet API
TCP 10255 - Read-only Kubelet API
TCP 30000-32767 - NodePort Services

Service Networking
kubectl get pods -o wide
kubectl get deployments
service networking exposes pods to the outside world
kubectl expose deployment webhead --type="NodePort" --port=80
kubectl get services
curl localhost:[port listed above]

Ingress
Ingress is an API object that manages external access to the services in a cluster, usually HTTP
It can provide load balancing, SSL termination, and name-based virtual hosting.
For our purposes, an Edge router is a router that enforces the firewall policy of your cluster.
This could be a gateway managed by a cloud provider or a physical piece of hardware.
Our cluster network is a set of links, either logical or physical, that facilitate communication within a
  cluster according to the Kubernetes networking model. Examples of a Cluster network include
  Overlays such as flannel, like we're using in our Linux Academy Cloud server cluster, or SDNs such as OpenVSwitch
A Service is a Kubernetes Service that identifies a set of pods using label selectors.
Unless mentioned otherwise, Services are assumed to have virtual IPs only routable within the cluster network.
What is Ingress?
Services and Pods have IPs only routable by the cluster network
So an Ingress is a collection of rules that allow inbound connections.
Can be configured to give services externally-reachable URLs, load balance traffic, terminate SSL,
  offers name-based virtual hosting, and the like.
Users request ingress by POSTing the Ingress resources to the API server. An Ingress controller is
  responsible for fulfilling the Ingress, usually by way of a load balancer, though it may also configure the
  edge router or additional front ends to help handle the traffic in a Highly Available manner.
Relatively new resource and not available in any Kubernetes release prior to 1.1
Ingress controller to satisfy an Ingress object
Most cloud providers deploy an ingress controller on the master.
Each ingress pod must be annotated with the appropriate class.
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
  annotations:
    ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /path
        backend:
          serviceName: test
          servicePort: 80

cluster must have an ingress controller running for ingresses to work
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
spec:
  backend:
    serviceName: testsvc
    servicePort: 80

kubectl create -f <filename.yaml>
kubectl get ing
NAME            RULE   BACKEND      ADDRESS
test-ingress    -      testsvc:80   107.178.254.228
Where 107.178.254.228 is the IP allocated by the Ingress controller to satisfy this Ingress. The RULE
  column shows that all traffic to the IP is directed to the Kubernetes Service listed under BACKEND

---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test
  annotations: ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: some.example.com
    http:
      paths:
      - path: /service1
        backend:
          serviceName: s1
          servicePort: 80
      - path: /service2
        backend:
          serviceName: s2
          servicePort: 80
requests to some.example.com/service1 go to the service s1
and reqs to some.example.com/service2 go to service s2

kubectl get ing
NAME RULE         BACKEND ADDRESS
test -
     some.example.com
     /service1    s1:80
     /service2    s2:80

---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test
spec:
  rules:
  - host: service1.example.com
    http:
      paths:
      - backend:
          serviceName: s1
          servicePort: 80
  - host: service2.example.com
    http:
      paths:
      - backend:
          serviceName: s2
          servicePort: 80

tells backend to route requests based on host header
an ingress with no rules will send all traffic to single default backend
can be used for serving 404 page

How to secure an Ingress:
Specify secret
  TLS private key
  Certificate
Port 443 (Assumes TLS Termination)
Multiple hosts are multiplexed on the same port by hostnames specified through the SNI TLS extension
The TLS secret must contain keys named tls.crt and tls.key that contain the certificate
  and private key to use for TLS
---
apiVersion: v1
data:
  tls.crt: base64 encoded cert
  tls.key: base64 encoded key
kind: Secret
metadata:
  name: supersecret
  namespace: default
type: Opaque
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: no-rules-map
spec:
  tls:
  - secretName: supersecret
  backend:
    serviceName: s1
    servicePort: 80

An Ingress controller is bootstrapped with a load balancing policy that it applies to all Ingress objects
  (e.g. load balancing algorithm, backend weight scheme, etc.)
Persistent sessions and dynamic weights not yet exposed
The service load balancer may provide some of this
Health checks are not exposed directly through the Ingress
Readiness probes allow for similar functionality

can also update a running ingress
kubectl get ing
kubectl edit ing test -> opens editor and can add a host
Alternatively, kubectl replace -f on a modified Ingress yaml file.
This command updates a running Kubernetes object.

Remember that Ingress is a relatively new concept in Kubernetes
Other ways to expose a service that doesn't directly involve the Ingress resource:
Use Service.Type=LoadBalancer
Use Service.Type=NodePort
Use a Port Proxy

Deploying a Load Balancer
---
kind: Service
apiVersion: v1
metadata:
  name: la-lb-service
spec:
  selector:
    app: la-lb
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
  clusterIP: 10.0.171.223
  loadBalancerIP: 78.12.23.17
  type: LoadBalancer

Configure and Use Cluster DNS
allows us to use hostnames from applications to connect to other services
handled by kube-dns
kubectl get pods -n kube-system
kubernetes get pods -> got a busybox pod running
kubernetes get services -> kubernetes and webhead are running
kubectl exec -it busybox -- nslookup kubernetes -> looks up kubernetes address from within busybox
kubectl get deployments -> there is one called dns-target
kubectl exec -it busybox -- nslookup dns-target -> cant find anything
kubectl expose deployment dns-target -> creates a service from the deployment, so it can be found by other pods
kubectl exec -it busybox -- nslookup dns-target -> finds the dns-target service
could also use a configMap to pass the addresses of upstream servers to individual pods
Troubleshooting:
check dns resolution as local level
  kubectl exec -it busybox -- cat /etc/resolv.conf
check that dns pod is running
  kubectl get pods -n kube-system
check logs in dns pod
  kubectl logs -n kube-system $(kubectl get pods -n kube-system -l k8s-app=kube-dns -o name) -c kubedns
  kubectl logs -n kube-system $(kubectl get pods -n kube-system -l k8s-app=kube-dns -o name) -c dnsmasq
  kubectl logs -n kube-system $(kubectl get pods -n kube-system -l k8s-app=kube-dns -o name) -c sidecar
kubectl get svc -n kube-system
kubectl get endpoints kube-dns -n kube-system

Container Network Interface
All pods can communicate with all other pods
Each pod has its own IP address
No need for mapping container ports
Backward compatible model with VMs
  Port allocation
  Naming
  Service discovery
  Load balancing
  Application Configuration
  Migration

Kubernetes Network Model
Dynamic Port Allocation Problems:
  Every application must be configured to know which ports, etc.
  API services must inject dynamic port numbers into containers
  Services must be able to find one another or be configured to do so
The Kubernetes Way
  All containers can communicate with each other without NAT
  All nodes can communicate with all containers (& vice versa) without NAT
  The IP of a container is the same regardless of which container views it
  K8s applies IP addresses at the pod level
  "IP-per-Pod" -- Containers in a pod share a single IP address, like processes in a VM
But...How?
CNI - Container Network Interface
  Must be implemented as an executable invoked by the container management system (in our case, Kubernetes)
  Plugin is responsible for:
    Inserting the network interface into the container network namespace
    Making necessary changes to the host
    Assign IP addresses to the interface
    Set up routes consistent with IP address management
Kubelet
Default network plugin
Default cluster-wide network
Probes for network plugins on startup

Flannel
Simple and easy Layer 3 network fabric
flanneld runs on each host (via a DaemonSet)
  Allocates subnet lease to each host
  Stores network configuration, allocated subnets, other data
  Packets forwarded using VxLANs
  Does not offer support for network policies

Calico
Free and Open Source
Simplified networking model
Scalable, distributed control plane
Policy-driven network security
Uses overlay networks sparingly
Widely deployed
Can be run in policy enforcement mode

Other worth mentioning
Cilium
Contiv
Contrail
Multus
NSX-T
Nuage Networks VCS
OpenVSwitch
OVN
Romana
Weave Net
CNI-Genie

Kubernetes requires its networking model to be implemented by a third-party plugin, called a CNI
Different CNIs feature support for different hardware, software, overlay networks, policies, and features
Administrators must select a CNI appropriate to their environment

Quiz
Think about the YAML for a network policy. If you had to create one, what is the pattern?
  Preamble, podSelector, ingress, and/or egress rules
  Preamble contains apiVersion, Kind, and Metadata; then comes the podSelector to determine which pods this policy oversees; and, finally, the rules.

What is an Ingress as it relates to Kubernetes?
  An API object that manages external access to the services in a cluster, usually HTTP.
  A fairly new concept in Kubernetes, an Ingress allows us to abstract away the implementation details of routes into the cluster, such as Load Balancers.

When a service type of "ClusterIP" is used, what is the result?
  A single IP address within the cluster that redirects traffic to a pod (possibly on a different node) serving the application (the pod).
  ClusterIP is most commonly used with third-party load balancers.

Exercise: Cluster DNS & Service Discovery
In a Kubernetes cluster, services discover one another through the Cluster DNS.  Names of services resolve to their ClusterIP, allowing application developers to only know the name of the service deployed in the cluster and not have to figure out how to get all the right IP addresses into the right containers at deploy time.

Here is yaml for a deployment:

apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: bit-of-nothing
spec:
  selector:
    matchLabels:
      app: pause
  replicas: 2
  template:
    metadata:
      labels:
        app: pause
    spec:
      containers:
      - name: bitty
        image: k8s.gcr.io/pause:2.0
Create this file and name it bit-of-nothing.yaml.

1. Run the deployment and verify that the pods have started.
  kubectl create -f bit-of-nothing.yaml

2. Start a busybox pod you can use to check DNS resolution (you can use my yaml, below, if you like, but it's good practice to write your own!)
  kubectl create -f busybox-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - name: busybox
    image: busybox:1.28
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always
3. Check to see whether or not bit-of-nothing is currently being resolved in the cluster via your busybox container.
  kubectl exec -it busybox -- nslookup bit-of-nothing

4. Expose the bit-of-nothing deployment as a ClusterIP service.
  kubectl expose deployment bit-of-nothing --type=clusterIP --port 80

5. Verify that "bit-of-nothing" is now being resolved to an IP address in your cluster.
  kubectl exec -it busybox -- nslookup bit-of-nothing

Kubernetes and Persistent Volumes
Native Pod Storage is Ephemeral -- Like a Pod
What happens when a container crashes:
  Kubelet restarts it (possibly on another node)
  File system is recreated from image
  Ephemeral files are gone
Docker Volumes
  Directory on disk
  Possibly in another container
  New Volume Drivers
Kubernetes Volumes
  Same lifetime as its pod
  Data preserved across container restarts
  Pod goes away -> Volume goes away
  Directory with data
  Accessible to containers in a pod
  Implementation details determined by volume types
Using volumes
  Pod spec indicates which volumes to provide for the pod (spec.volumes)
  Pod spec indicates where to mount these volumes in containers (spec.containers.volumeMounts)
  Seen from the container's perspective as the file system
  Volumes cannot mount onto other volumes
  No hard links to other volumes
  Each pod must specify where each volume is mounted
Kubernetes Supported Volume Types
  awsElasticBlockStore
    Mounts an AWS EBS volume to a pod
    EBS volume is preserved when unmounted
    Must be created prior to use
    Nodes must be on AWS EC2 instances in the same region
    Single instance mounting only
    Created via a command like:
    aws ec2 create-volume --availability-zone=eu-west-la --size=10 --volume-type=gp2
    example yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-ebs
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-ebs
      name: test-volume
    volumes:
    - name: test-volume
      # this AWS EBS volume must already exist
      awsElasticBlockStore:
        volumeID: <volume-id>
        fsType: ext4

azureDisk and azureFile
  An azureDisk is used to mount a Microsoft Azure Data Disk into a pod.
  An azureFile is used to mount a Microsoft Azure File Volume (SMB 2.1 and 3.0) into a pod.
cephfs
  Allows mounting a CephFS volume to a pod
  Contents of volume are preserved when unmounted
  Must have a Ceph server running
  Share must be exported
csi
  Container Storage Interface
  In-tree CSI volume plugin for volumes on the same node
  Kubernetes 1.9+
    --feature-gates=CSIPersistentVolume=true
  Metadata field specify what is used and how
  Driver fields specify the name of the driver
  volumeHandle identifies volume name
  readOnly is supported
downwardAPI
  Mounts a directory and writes data in plain text files
emptyDir
  Created when a pod is assigned to a node
  Exists while pod runs on a particular node
  Initially empty
  Multiple containers can read/write the same volume
  Volume can be mounted per container -- same or different mount points
  Pod removed -> volume removed
  Stored on node's local medium
  Optional - set emptyDir.medium = Memory for RAM based tmpfs

