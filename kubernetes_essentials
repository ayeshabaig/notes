Kubernetes Essentials
kubernetes.io

Cluster Architecture

Kube Master
  Docker
  Kubeadm
  Kubelet
  Kubectl
  Control Plane

Kube Node 1
  Docker
  Kubeadm
  Kubelet
  Kubectl

Kube Node 2
  Docker
  Kubeadm
  Kubelet
  Kubectl

Setting up Playground Servers
New Server
Ubuntu 18.04
Small
Tag Kube Master, Kube Node 1, Kube Node 2
ssh into servers

Installing Docker
Docker is a container runtime which is the software that actually runs the containers.
Add the Docker repo GPG key
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
Add the Docker repo
sudo add-apt-repository    "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"
Reload the apt sources list
sudo apt-get update
Install Docker
sudo apt-get install -y docker-ce=18.06.1~ce~3-0~ubuntu
Prevent auto-updates for the Docker package
sudo apt-mark hold docker-ce
Verify that Docker is working
sudo docker version

Installing Kubeadm, Kubelet, and Kubectl
Kubeadm - automates a large portion of the process of setting up a cluster
Kubelet - essential part of kubernetes that handles running containers on a node, every running server needs kubelet
Kubectl - CLI tools for interacting with the cluster once it is up

Do the following to each server:

Install the Kubernetes GPG key
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

Add the Kubernetes repo
cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF

Reload the apt sources list
sudo apt-get update

Install packages (use specific version to make it easier to follow along)
sudo apt-get install -y kubelet=1.12.2-00 kubeadm=1.12.2-00 kubectl=1.12.2-00

Prevent auto-updates for the Kube packages
sudo apt-mark hold kubelet kubeadm kubectl

Verify that kubeadm is working
kubeadm version

Bootstrapping the cluster
Kube master node, initialize cluster
sudo kubeadm init --pod-network-cidr=10.244.0.0/16

Setup local kubeconfig
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Verify the cluster is responsive and Kubectl is working
kubectl version - should get Server Version and Client Version

kubeadm init command should output a kubeadm join command with token and hash to be run with sudo on both worker nodes
something like this:
sudo kubeadm join $some_ip:6443 --token $some_token --discovery-token-ca-cert-hash $some_hash

Verify all nodes successfully joined cluster
kubectl get nodes

Configuring Networking using Flannel
On all nodes run
echo "net.bridge.bridge-nf-call-iptables=1" | sudo tee -a /etc/sysctl.conf
sudo sysctl -p

Install Flannel on the master node
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml

Verify that all nodes have a status of ready
kubectl get nodes

Verify Flannel pods are up and running
kubectl get pods -n kube-system

Containers and Pods

Pods are the smallest and most basic building blocks of the Kubernetes model

A pod consists of one or more containers, storage resources, and a unique IP address in the Kubernetes cluster network

In order to run containers, Kubernetes schedules pods to run on servers in the cluster.
When a pod is scheduled, the server will run the containers that are part of the pod.

Create a simple pod running an nginx container:
cat << EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
EOF

get a list of pods in default namespace and verify that the nginx pod is running
kubectl get pods

to get all pods in all namespaces
kubectl get pods --all-namespaces

to see the kube pods
kubectl get pods -n kube-system

for more info about the pod
kubectl describe pod nginx
or more generally
kubectl describe pod $pod_name -n $namespace

delete the pod
kubectl delete pod nginx

Clustering and Nodes
Kubernetes implements clustered architecture
typical env, multiple servers to run containers

Nodes - Servers that run containers
Control servers - manage and control the cluster and host the kube API
Worker nodes - run applications within the cluster

get a list of nodes-
kubectl get nodes

get more info about a specific node-
kubectl describe node $node_name

Networking in Kubernetes
The Kubernetes networking model involves creating a virtual network across the whole cluster.
Every pod on the cluster has a unique IP address, and can communicate with any other pod in the cluster,
even if the other pode is running on a different node

Example
Create a deployment with 2 nginx pods:
cat << EOF | kubectl create -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.15.4
        ports:
        - containerPort: 80
EOF

Create a busybox pod to use for testing:
cat << EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - name: busybox
    image: radial/busyboxplus:curl
    args:
    - sleep
    - "1000"
EOF

Get the IP addresses of your pods
kubectl get pods -o wide

Get the IP address of one of the nginx pods, then contact that nginx pod from the busybox pod using the nginx pod's IP address
kubectl exec busybox -- curl $nginx_pod_ip

Kubernetes Architecture and Components
kubectl get pods -n kube-system

Kubernetes includes mutliple components that work together to provide the functionality of a Kube cluster.

The control plane components manage and control the cluster:
- etcd - Provides distributed, synchronized data storage for the cluster state
- kube-apiserver - Serves the Kubernetes API, the primary interface for the cluster
- kube-controller-manager - Bundles several components into one package
- kube-scheduler - Schedules pods to run on individual nodes

In addition to the control plane, each node also has:
- kubelet - Agent that executes containers on each node
- kube-proxy - Handles network communication between nodes by adding firewall rules

With kubeadm, many of these components are run as pods within the cluster itself

Check the status of the kubelet service
sudo systemctl status kubelet

Kubernetes Deployments
A deployment allows you to specify a desired state for a set of pods. Then cluster will then constantly work to maintain that desired state.
For example:
- Scaling - With a deployment, you can specify the number of replicas you want, and the deployment will create (or remove) pods to meet that number of replicas.
- Rolling Updates - With a deployment, you can change the deployment container image to a new version of the image.
    The deployment will gradually replace existing containers with the new version.
- Self-Healing - if one of the pods in the deployment is accidentally destroyed, the deployment will immediately spin up a new one to replace it.

Create a deployment:
cat <<EOF | kubectl create -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.15.4
        ports:
        - containerPort: 80
EOF

To get a list of deployments:
kubectl get deployments

Get more info about a deployment:
kubectl describe deployment nginx-deployment

Get a list of pods
kubectl get pods
The deployment should have created 2 pods

Kubernetes Services

Services allow you to dynamically access a group of replica pods.
Replica pods are often being created and destroyed, so what happens to other pods
or external entities which need to access those pods?

A Service creates an abstraction layer on top of a set of replica pods.
You can access the service rather than accessing the pods directly,
so as pods come and go, you get uninterrupted, dynamic access to whatever replicas are up at the time.

Create a NodePort service on top of your nginx pods:
cat << EOF | kubectl create -f -
kind: Service
apiVersion: v1
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30080
  type: NodePort
EOF

Get a list of services in the cluster:
kubectl get svc or kubectl get services - you should see a service called nginx-service

Test that NodePort is accessible
curl localhost:30080
you should get a response from nginx

Learning Activity
Set up a store-products deployment and service
cat <<EOF | kubectl create -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: store-products
  labels:
    app: store-products
spec:
  replicas: 4
  selector:
    matchLabels:
      app: store-products
  template:
    metadata:
      labels:
        app: store-products
    spec:
      containers:
      - name: store-products
        image: linuxacademycontent/store-products:1.0.0
        ports:
        - containerPort: 80
EOF

Create a store-products service
cat <<EOF | kubectl create -f -
kind: Service
apiVersion: v1
metadata:
  name: store-products
spec:
  selector:
    app: store-products
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
EOF

kubectl get svc store-products -> see that the service exists
use busybox to see that the service is working
kubectl exec busybox -- curl -s store-products

Mircroservices are small, independent services that work together to form a whole application
Many applications are designed with a monolithic architecture,
meaning that all parts of the application are combined in one large executable.

Advantages of microservices:
Scalability - individual microservices are independently scalable
Cleaner code - it's easier to make changes in one area of the application without breaking things in other areas
Reliability - Problems in one area are less likely to affect other areas
Variety of tools - different parts of the app can be built with different tools, languages, and frameworks

Implementing microservices means deploying, scaling, and managing a lot of individual components. Kubernetes is great for this

Deploying the Robot Shop App
Stan's Robot Shop repo from Instana

Clone the git repo
cd ~/
git clone https://github.com/linuxacademy/robot-shop.git

Create a namespace and deploy the application objects to the namespace using the deployment descriptors from the Git repo
kubectl create namespace robot-shop
kubectl -n robot-shop create -f ~/robot-shop/K8s/descriptors

Get a list of the application's pods and wait for all of them to finish starting up
kubectl get pods -n robot-shop -w

Once all the pods are up, you can access the application in a browser using the public IP of one of your Kubernetes servers and port 30080
http://$kube_server_public_ip:30080

To scale the number of mongo replicas:
kubectl edit deployment mongodb -n robot-shop
change replicas: 1 to replicas: 2
kubectl get deployment mongodb -n robot-shop -> check status of deployment

